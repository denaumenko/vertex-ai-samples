{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Vertex AI - Llama2 fine-tuning with LoRA and serving on TPUv5e\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/training/tpuv5e_llama2_pytorch_finetuning_and_serving.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"><br> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fofficial%2Ftraining%2Ftpuv5e_llama2_pytorch_finetuning_and_serving.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://cloud.google.com/ml-engine/images/colab-enterprise-logo-32px.png\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/training/tpuv5e_llama2_pytorch_finetuning_and_serving.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br>\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/notebooks/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/training/tpuv5e_llama2_pytorch_finetuning_and_serving.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br>\n",
    "Open in Vertex AI Workbench\n",
    "    </a> \n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9e4695f5e9dc"
   },
   "source": [
    "## Quota - Make sure this is complete before you start!\n",
    "\n",
    "In order to run this example, you will need the following TPUv5e quota approved. You can make requests in the console via IAM & Admin > Quotas or by reaching out to your Google account team:\n",
    "\n",
    "- aiplatform.googleapis.com/custom_model_serving_tpu_v5e (4-8 chips. 4 chips minimum for Llama2 7B)\n",
    "- aiplatform.googleapis.com/custom_model_training_tpu_v5e (16 chips minimum)\n",
    "\n",
    "Check the [TPU pricing page](https://cloud.google.com/tpu/pricing) for the region availability and pricing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates fine-tuning of a Llama2 7B model with [LoRA](https://huggingface.co/docs/peft/v0.9.0/en/package_reference/lora#peft.LoraConfig), and uses a TPUv5e for both fine-tuning and serving. The fine-tuning is based on a [Hugging Face example](https://huggingface.co/google/gemma-7b/blob/main/examples/example_fsdp.py) that uses [fully sharded data parallel with PyTorch XLA](https://pytorch.org/blog/scaling-pytorch-models-on-cloud-tpus-with-fsdp/), and [SPMD](https://pytorch.org/blog/pytorch-xla-spmd/). Follow the links to learn more.\n",
    "\n",
    "\n",
    "Fine-tuning occurs with a [Vertex AI Custom Training Job](https://cloud.google.com/vertex-ai/docs/training/create-custom-job). A Vertex AI Custom Training Job allows for a higher level of customization and control over the fine-tuning job. All of the examples in this notebook use parameter Low-Rank Adaption [LoRA](https://huggingface.co/docs/peft/en/package_reference/lora) to reduce training and storage costs.\n",
    "\n",
    "This notebook deploys the model using Hex-LLM, a High-Efficiency Large Language Model serving solution built with XLA that is being developed by Google Cloud\n",
    "\n",
    "\n",
    "### Objective\n",
    "\n",
    "- Fine-tune and deploy Llama2 models with a Vertex AI Custom Training Job and Vertex Prediction endpoint.\n",
    "- Send prediction requests to your fine-tuned Llama2 model.\n",
    "\n",
    "\n",
    "### Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI (Training, Prediction, TPUv5e)\n",
    "* Cloud Storage\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing), [Cloud NL API pricing](https://cloud.google.com/natural-language/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08d289fa873f"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "In this example, you will use an english_quotes dataset from Hugging Face to fine-tune the model. Details of the dataset can be found here: https://huggingface.co/datasets/Abirate/english_quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "## Get started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81711614c199"
   },
   "source": [
    "### Install Vertex AI SDK for Python and other required packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "2b4ef9b72d43"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# (optional) update gcloud if needed\n",
    "if os.getenv(\"IS_TESTING\"):\n",
    "    ! gcloud components update --quiet\n",
    "\n",
    "! pip3 install --upgrade --quiet google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58707a750154"
   },
   "source": [
    "### Colab only: Uncomment the following cell to restart the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f200f10a1da3"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffcde4d56c00"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7176ea64999b"
   },
   "source": [
    "### Authenticate your notebook environment (Colab only)\n",
    "\n",
    "Authenticate your environment on Google Colab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7de6ef0fac42"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef3990d0482a"
   },
   "source": [
    "### Set Google Cloud project information and initialize Vertex AI SDK for Python\n",
    "\n",
    "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dac26e23d459"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"ml-project-461521\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "Create a storage bucket to store intermediate artifacts such as datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = f\"gs://liama2-tpuv5e-{PROJECT_ID}-unique\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EcIXiGsCePi"
   },
   "source": [
    "**If your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://liama2-tpuv5e-ml-project-461521-unique/...\n"
     ]
    }
   ],
   "source": [
    "! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "648eb19b83dd"
   },
   "source": [
    "#### Set folder paths for staging, environment, and model artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "855d6b96f291"
   },
   "outputs": [],
   "source": [
    "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
    "\n",
    "# The service account looks like:\n",
    "# '@.iam.gserviceaccount.com'\n",
    "# Please go to https://cloud.google.com/iam/docs/service-accounts-create#iam-service-accounts-create-console\n",
    "# and create service account with `Vertex AI User` and `Storage Object Admin` roles.\n",
    "# The service account for deploying fine tuned model.\n",
    "SERVICE_ACCOUNT = \"190345877179-compute@developer.gserviceaccount.com\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "960505627ddf"
   },
   "source": [
    "### Initialize Vertex AI SDK for Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION, staging_bucket=STAGING_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2931d2e5a94"
   },
   "source": [
    "### Access Llama 2 pretrained and finetuned models\n",
    "The original models from Meta are converted into the Hugging Face format for finetuning and serving in Vertex AI.\n",
    "\n",
    "Accept the model agreement to access the models:\n",
    "\n",
    "1. Navigate to the Vertex AI > Model Garden page in the Google Cloud console\n",
    "2. Search for Llama 2\n",
    "3. Review the agreement that pops up on the model card page\n",
    "4. Accept the agreement of Llama 2\n",
    "5. On the documentation tab, a Cloud Storage bucket containing Llama 2 pretrained and finetuned models will be shared\n",
    "5. Paste the Cloud Storage bucket link below and assign it to VERTEX_AI_MODEL_GARDEN_LLAMA2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "47330c9cae9f"
   },
   "outputs": [],
   "source": [
    "VERTEX_AI_MODEL_GARDEN_LLAMA2 = \"gs://vertex-model-garden-public-us-central1/llama2\"  # This will be shared once click the agreement of LLaMA2 in Vertex AI Model Garden.\n",
    "VERTEX_MODEL_ID = \"llama2-7b-hf\"\n",
    "HF_MODEL_ID = \"meta-llama/Llama-2-7b-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "890b49a97355",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy Llama 2 model artifacts from gs://vertex-model-garden-public-us-central1/llama2 to  gs://liama2-tpuv5e-ml-project-461521-unique/meta-llama/Llama-2-7b-hf\n",
      "Copying gs://vertex-model-garden-public-us-central1/llama2/llama2-7b-hf/LICENSE to gs://liama2-tpuv5e-ml-project-461521-unique/meta-llama/Llama-2-7b-hf/LICENSE\n",
      "Copying gs://vertex-model-garden-public-us-central1/llama2/llama2-7b-hf/LLaMA V2 Model Preview User Guide.pdf to gs://liama2-tpuv5e-ml-project-461521-unique/meta-llama/Llama-2-7b-hf/LLaMA V2 Model Preview User Guide.pdf\n",
      "Copying gs://vertex-model-garden-public-us-central1/llama2/llama2-7b-hf/MODEL_CARD.md to gs://liama2-tpuv5e-ml-project-461521-unique/meta-llama/Llama-2-7b-hf/MODEL_CARD.md\n",
      "Copying gs://vertex-model-garden-public-us-central1/llama2/llama2-7b-hf/Notice-File.docx to gs://liama2-tpuv5e-ml-project-461521-unique/meta-llama/Llama-2-7b-hf/Notice-File.docx\n",
      "Copying gs://vertex-model-garden-public-us-central1/llama2/llama2-7b-hf/Responsible-Use-Guide.pdf to gs://liama2-tpuv5e-ml-project-461521-unique/meta-llama/Llama-2-7b-hf/Responsible-Use-Guide.pdf\n",
      "Copying gs://vertex-model-garden-public-us-central1/llama2/llama2-7b-hf/USE_POLICY.md to gs://liama2-tpuv5e-ml-project-461521-unique/meta-llama/Llama-2-7b-hf/USE_POLICY.md\n",
      "Copying gs://vertex-model-garden-public-us-central1/llama2/llama2-7b-hf/config.json to gs://liama2-tpuv5e-ml-project-461521-unique/meta-llama/Llama-2-7b-hf/config.json\n",
      "Copying gs://vertex-model-garden-public-us-central1/llama2/llama2-7b-hf/config.json.bak to gs://liama2-tpuv5e-ml-project-461521-unique/meta-llama/Llama-2-7b-hf/config.json.bak\n",
      "Copying gs://vertex-model-garden-public-us-central1/llama2/llama2-7b-hf/generation_config.json to gs://liama2-tpuv5e-ml-project-461521-unique/meta-llama/Llama-2-7b-hf/generation_config.json\n",
      "Copying gs://vertex-model-garden-public-us-central1/llama2/llama2-7b-hf/pytorch_model-00001-of-00002.bin to gs://liama2-tpuv5e-ml-project-461521-unique/meta-llama/Llama-2-7b-hf/pytorch_model-00001-of-00002.bin\n",
      "Copying gs://vertex-model-garden-public-us-central1/llama2/llama2-7b-hf/pytorch_model-00002-of-00002.bin to gs://liama2-tpuv5e-ml-project-461521-unique/meta-llama/Llama-2-7b-hf/pytorch_model-00002-of-00002.bin\n",
      "Copying gs://vertex-model-garden-public-us-central1/llama2/llama2-7b-hf/pytorch_model.bin.index.json to gs://liama2-tpuv5e-ml-project-461521-unique/meta-llama/Llama-2-7b-hf/pytorch_model.bin.index.json\n",
      "Copying gs://vertex-model-garden-public-us-central1/llama2/llama2-7b-hf/special_tokens_map.json to gs://liama2-tpuv5e-ml-project-461521-unique/meta-llama/Llama-2-7b-hf/special_tokens_map.json\n",
      "Copying gs://vertex-model-garden-public-us-central1/llama2/llama2-7b-hf/tokenizer.json to gs://liama2-tpuv5e-ml-project-461521-unique/meta-llama/Llama-2-7b-hf/tokenizer.json\n",
      "Copying gs://vertex-model-garden-public-us-central1/llama2/llama2-7b-hf/tokenizer.model to gs://liama2-tpuv5e-ml-project-461521-unique/meta-llama/Llama-2-7b-hf/tokenizer.model\n",
      "Copying gs://vertex-model-garden-public-us-central1/llama2/llama2-7b-hf/tokenizer_config.json to gs://liama2-tpuv5e-ml-project-461521-unique/meta-llama/Llama-2-7b-hf/tokenizer_config.json\n",
      "  Completed files 16/16 | 12.6GiB/12.6GiB                                      \n",
      "\n",
      "Average throughput: 90.2GiB/s\n"
     ]
    }
   ],
   "source": [
    "assert (\n",
    "    VERTEX_AI_MODEL_GARDEN_LLAMA2\n",
    "), \"Please click the agreement of Llama 2 in Vertex AI Model Garden, and get the GCS path of Llama 2 model artifacts.\"\n",
    "print(\n",
    "    \"Copy Llama 2 model artifacts from\",\n",
    "    VERTEX_AI_MODEL_GARDEN_LLAMA2,\n",
    "    \"to \",\n",
    "    f\"{BUCKET_URI}/{HF_MODEL_ID}\",\n",
    ")\n",
    "\n",
    "# Copy model files to your bucket\n",
    "! gcloud storage cp -R $VERTEX_AI_MODEL_GARDEN_LLAMA2/$VERTEX_MODEL_ID/* $BUCKET_URI/$HF_MODEL_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "832bbd8cb9ef"
   },
   "source": [
    "### Create the artifact registry repository and set the custom docker image uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "18fd49b88127"
   },
   "outputs": [],
   "source": [
    "REPOSITORY = \"tpuv5e-liama2-7b-training-repository-unique\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "c2242d87effa"
   },
   "outputs": [],
   "source": [
    "image_name_train = \"llama2-7b-hf-lora-tuning-tpuv5e\"\n",
    "hostname = f\"{LOCATION}-docker.pkg.dev\"\n",
    "tag = \"latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "9dd94acb26e9",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mWARNING:\u001b[0m Your config file at [/home/jupyter/.docker/config.json] contains these credential helper entries:\n",
      "\n",
      "{\n",
      "  \"credHelpers\": {\n",
      "    \"gcr.io\": \"gcloud\",\n",
      "    \"us.gcr.io\": \"gcloud\",\n",
      "    \"eu.gcr.io\": \"gcloud\",\n",
      "    \"asia.gcr.io\": \"gcloud\",\n",
      "    \"staging-k8s.gcr.io\": \"gcloud\",\n",
      "    \"marketplace.gcr.io\": \"gcloud\",\n",
      "    \"us-west1-docker.pkg.dev\": \"gcloud\",\n",
      "    \"us-central1-docker.pkg.dev\": \"gcloud\"\n",
      "  }\n",
      "}\n",
      "Adding credentials for: us-central1-docker.pkg.dev\n",
      "gcloud credential helpers already registered correctly.\n"
     ]
    }
   ],
   "source": [
    "# Register gcloud as a Docker credential helper\n",
    "!gcloud auth configure-docker $LOCATION-docker.pkg.dev --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "5c8e8364a097"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create request issued for: [tpuv5e-liama2-7b-training-repository-unique]\n",
      "Waiting for operation [projects/ml-project-461521/locations/us-central1/operati\n",
      "ons/9a7328c3-ad2c-4a75-b625-1980b3dd08ce] to complete...done.                  \n",
      "Created repository [tpuv5e-liama2-7b-training-repository-unique].\n"
     ]
    }
   ],
   "source": [
    "# One time or use an existing repository\n",
    "!gcloud artifacts repositories create $REPOSITORY --repository-format=docker \\\n",
    "--location=$LOCATION --description=\"Vertex TPUv5e training repository\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "16488f1fc702"
   },
   "outputs": [],
   "source": [
    "# Define container image name\n",
    "PYTORCH_TRAIN_DOCKER_URI = (\n",
    "    f\"{hostname}/{PROJECT_ID}/{REPOSITORY}/{image_name_train}:{tag}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0c250872074f"
   },
   "source": [
    "### Define common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "354da31189dc"
   },
   "outputs": [],
   "source": [
    "def get_job_name_with_datetime(prefix: str) -> str:\n",
    "    \"\"\"Gets the job name with date time when triggering training or deployment\n",
    "    jobs in Vertex AI.\n",
    "    \"\"\"\n",
    "    return prefix + datetime.now().strftime(\"_%Y%m%d_%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e11e1f8a6c4c"
   },
   "source": [
    "### Build the Docker container files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19d584f9fe62"
   },
   "source": [
    "#### Create the trainer directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "909e93e7fd43"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(\"trainer2\"):\n",
    "    os.makedirs(\"trainer2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8a91ddf0cbcb"
   },
   "source": [
    "#### Create the Dockerfile for the custom container. This will install Hugging Face transformers, datasets, trl, and peft for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "756577886992"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing trainer2/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile trainer2/Dockerfile\n",
    "# This Dockerfile fine tunes the Llamas2 model using LoRA with PyTorch XLA\n",
    "# Nightly TPU VM docker image\n",
    "FROM us-central1-docker.pkg.dev/tpu-pytorch-releases/docker/xla:nightly_3.10_tpuvm_20240324\n",
    "\n",
    "ENV DEBIAN_FRONTEND=noninteractive\n",
    "\n",
    "# Install basic libs\n",
    "RUN apt-get update && apt-get -y upgrade && apt-get install -y --no-install-recommends \\\n",
    "        cmake \\\n",
    "        curl \\\n",
    "        wget \\\n",
    "        sudo \\\n",
    "        gnupg \\\n",
    "        libsm6 \\\n",
    "        libxext6 \\\n",
    "        libxrender-dev \\\n",
    "        lsb-release \\\n",
    "        ca-certificates \\\n",
    "        build-essential \\\n",
    "        git \\\n",
    "        libgl1\n",
    "\n",
    "# Copy Apache license.\n",
    "RUN wget https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/LICENSE\n",
    "\n",
    "# Install required libs\n",
    "RUN pip install --upgrade pip\n",
    "RUN pip install --upgrade pip\n",
    "RUN pip install transformers==4.38.2 -U\n",
    "RUN pip install datasets==2.18.0\n",
    "RUN pip install trl==0.8.1 peft==0.10.0\n",
    "RUN pip install accelerate==0.28.0\n",
    "RUN pip install --upgrade google-cloud-storage\n",
    "\n",
    "# Copy other licenses.\n",
    "RUN wget -O MIT_LICENSE https://github.com/pytest-dev/pytest/blob/main/LICENSE\n",
    "RUN wget -O BSD_LICENSE https://github.com/pytorch/xla/blob/master/LICENSE\n",
    "RUN wget -O BSD-3_LICENSE https://github.com/pytorch/pytorch/blob/main/LICENSE\n",
    "\n",
    "# Copy install libtpu to PATH above\n",
    "RUN find ./usr/local/lib -name 'libtpu.so' -exec cp {} /lib \\;\n",
    "\n",
    "WORKDIR /\n",
    "COPY train.py train.py\n",
    "ENV PYTHONPATH ./\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ee7ac9469ce"
   },
   "source": [
    "#### Add the __init__.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "b0b57ecd02d8"
   },
   "outputs": [],
   "source": [
    "!touch trainer2/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c648296e1057"
   },
   "source": [
    "#### Add the train.py file\n",
    "This code is from the LoRA distributed fine-tuning code from this example: https://ai.google.dev/gemma/docs/distributed_tuning\n",
    "\n",
    "The IMDB TensorFlow dataset is used to fine-tune the Gemma model. Additional logic is added to handle the TPU topology setting required by TPUv5e: https://cloud.google.com/tpu/docs/v5e#tpu-v5e-config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "1b6a47a6089c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting trainer2/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile trainer2/train.py\n",
    "import os, sys\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch_xla\n",
    "import torch_xla.core.xla_model as xm\n",
    "\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "# use spmd\n",
    "import torch_xla.runtime as xr\n",
    "xr.use_spmd()\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--tpu_topology\",\n",
    "    help=\"Topology to use for the TPUv5e (2x2, 2x4, 4x4)\",\n",
    "    default=\"4x4\",\n",
    "    type=str\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--model_name\",\n",
    "    help=\"Llama2 model name (meta-llama/Llama-2-7b-hf, meta-llama/Llama-2-13b-hf)\",\n",
    "    default=\"meta-llama/Llama-2-7b-hf\",\n",
    "    type=str\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--bucket_name\",\n",
    "    help=\"The name of the bucket you copied the Llama2 model files to\",\n",
    "    required=True,\n",
    "    type=str\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--output_folder\",\n",
    "    type=str,\n",
    "    required=True,\n",
    "    help=\"Output folder name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--checkpoint_directory\",\n",
    "    type=str,\n",
    "    default=\"output_ckpt\",\n",
    "    help=\"Checkpoint Directory name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--epochs\",\n",
    "    type=int,\n",
    "    default=10,\n",
    "    help=\"Number of epochs to train\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--merged_model_folder\",\n",
    "    type=str,\n",
    "    default=\"llama2-7b-hf/modelfiles\",\n",
    "    help=\"Checkpoint Directory name\",\n",
    ")\n",
    "args = parser.parse_args()\n",
    "\n",
    "GCS_PREFIX = \"gs://\"\n",
    "\n",
    "def is_gcs_path(input_path: str) -> bool:\n",
    "    return input_path.startswith(GCS_PREFIX)\n",
    "\n",
    "def download_gcs_dir(gcs_dir: str, local_dir: str):\n",
    "    \"\"\"Download files in a GCS directory to a local directory.\n",
    "\n",
    "    For example:\n",
    "    download_gcs_dir(gs://bucket/foo, /tmp/bar)\n",
    "    gs://bucket/foo/a -> /tmp/bar/a\n",
    "    gs://bucket/foo/b/c -> /tmp/bar/b/c\n",
    "\n",
    "    Arguments:\n",
    "    gcs_dir: A string of directory path on GCS.\n",
    "    local_dir: A string of local directory path.\n",
    "    \"\"\"\n",
    "    if not is_gcs_path(gcs_dir):\n",
    "        raise ValueError(f\"{gcs_dir} is not a GCS path starting with gs://.\")\n",
    "\n",
    "    bucket_name = gcs_dir.split(\"/\")[2]\n",
    "    prefix = gcs_dir[len(GCS_PREFIX + bucket_name) :].strip(\"/\")\n",
    "    client = storage.Client()\n",
    "    blobs = client.list_blobs(bucket_name, prefix=prefix)\n",
    "    for blob in blobs:\n",
    "        if blob.name[-1] == \"/\":\n",
    "            continue\n",
    "        file_path = blob.name[len(prefix) :].strip(\"/\")\n",
    "        local_file_path = os.path.join(local_dir, file_path)\n",
    "        os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n",
    "        blob.download_to_filename(local_file_path)\n",
    "        print (f'download of {local_file_path} complete')\n",
    "    print (f'Show all files in directory {os.listdir(local_dir)}')\n",
    "\n",
    "def upload_directory_with_transfer_manager(bucket_name, source_directory, blob_name_prefix, workers=8):\n",
    "    \"\"\"Upload every file in a directory, including all files in subdirectories.\n",
    "\n",
    "    Each blob name is derived from the filename, not including the `directory`\n",
    "    parameter itself. For complete control of the blob name for each file (and\n",
    "    other aspects of individual blob metadata), use\n",
    "    transfer_manager.upload_many() instead.\n",
    "    \"\"\"\n",
    "\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "\n",
    "    # The directory on your computer to upload. Files in the directory and its\n",
    "    # subdirectories will be uploaded. An empty string means \"the current\n",
    "    # working directory\".\n",
    "    # source_directory=\"\"\n",
    "\n",
    "    # blob_name_prefix = prefix for the files being uploaded to GCS\n",
    "    # example: file1 and file2 in a folder uploaded to my-bucket with blob_name_prefix=my-folder/a/\n",
    "    # will be uploaded to gs://my-bucket/my-folder/a/file1 and gs://my-bucket/my-folder/a/file2\n",
    "    \n",
    "    # The maximum number of processes to use for the operation. The performance\n",
    "    # impact of this value depends on the use case, but smaller files usually\n",
    "    # benefit from a higher number of processes. Each additional process occupies\n",
    "    # some CPU and memory resources until finished. Threads can be used instead\n",
    "    # of processes by passing `worker_type=transfer_manager.THREAD`.\n",
    "    # workers=8\n",
    "\n",
    "    from pathlib import Path\n",
    "\n",
    "    from google.cloud.storage import Client, transfer_manager\n",
    "\n",
    "    storage_client = Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    # Generate a list of paths (in string form) relative to the `directory`.\n",
    "    # This can be done in a single list comprehension, but is expanded into\n",
    "    # multiple lines here for clarity.\n",
    "\n",
    "    # First, recursively get all files in `directory` as Path objects.\n",
    "    directory_as_path_obj = Path(source_directory)\n",
    "    paths = directory_as_path_obj.rglob(\"*\")\n",
    "\n",
    "    # Filter so the list only includes files, not directories themselves.\n",
    "    file_paths = [path for path in paths if path.is_file()]\n",
    "\n",
    "    # These paths are relative to the current working directory. Next, make them\n",
    "    # relative to `directory`\n",
    "    relative_paths = [path.relative_to(source_directory) for path in file_paths]\n",
    "\n",
    "    # Finally, convert them all to strings.\n",
    "    string_paths = [str(path) for path in relative_paths]\n",
    "\n",
    "    print(\"Found {} files.\".format(len(string_paths)))\n",
    "\n",
    "    # Start the upload.\n",
    "    print (f\"source directory {source_directory}\")\n",
    "    results = transfer_manager.upload_many_from_filenames(\n",
    "        bucket, string_paths, blob_name_prefix=blob_name_prefix, source_directory=source_directory, max_workers=workers\n",
    "    )\n",
    "\n",
    "    for name, result in zip(string_paths, results):\n",
    "        # The results list is either `None` or an exception for each filename in\n",
    "        # the input list, in order.\n",
    "\n",
    "        if isinstance(result, Exception):\n",
    "            print(\"Failed to upload {} due to exception: {}\".format(name, result))\n",
    "        else:\n",
    "            print(\"Uploaded {} to {}/{}.\".format(name, bucket.name, blob_name_prefix))\n",
    "    \n",
    "def main():\n",
    "    x = args.tpu_topology.split(\"x\")\n",
    "    tpu_topology_x = int(x[0])\n",
    "    tpu_topology_y = int(x[1])\n",
    "    print (f'TPU topology is ({tpu_topology_x}, {tpu_topology_y})')\n",
    "    print (f'Model name is {args.model_name}')\n",
    "    \n",
    "    # Set batch size to 8 for each chip\n",
    "    BATCH_SIZE = 8 * tpu_topology_x * tpu_topology_y\n",
    "    # For anything larger than an 8 chip instance, set the BATCH_SIZE to 128, since we run out of samples\n",
    "    if (tpu_topology_x * tpu_topology_y) >=16:\n",
    "        BATCH_SIZE = 128\n",
    "    \n",
    "    # Set download directory to a tempory folder\n",
    "    DL_DIR=\"/tmp/modelfiles\"\n",
    "    if not os.path.exists(DL_DIR):\n",
    "        os.makedirs(DL_DIR)\n",
    "\n",
    "    print ('Downloading data to temporary folder')\n",
    "    download_gcs_dir (f\"gs://{args.bucket_name}/{args.model_name}\", DL_DIR)\n",
    "    \n",
    "    # Create output folders\n",
    "    if not os.path.exists(f\"/tmp/{args.output_folder}\"):\n",
    "        os.makedirs(f\"/tmp/{args.output_folder}\")\n",
    "    if not os.path.exists(f\"/tmp/{args.checkpoint_directory}\"):\n",
    "        os.makedirs(f\"/tmp/{args.checkpoint_directory}\")\n",
    "\n",
    "    device = xm.xla_device()\n",
    "    \n",
    "    # Set tokenizer parallelism to false to avoid warnings\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(DL_DIR)\n",
    "    print ('Loaded tokenizer')\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(DL_DIR, torch_dtype=torch.bfloat16)\n",
    "    print ('Loaded base model')\n",
    "\n",
    "    # Set LoRA configuration\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"k_proj\", \"v_proj\"],\n",
    "    )\n",
    "    \n",
    "    # Required when using Llama2, as the tokenizer has no padding\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Load the dataset and format it for training.\n",
    "    data = load_dataset(\"Abirate/english_quotes\", split=\"train\")\n",
    "    max_seq_length = 512\n",
    "    print ('Loaded dataset')\n",
    "\n",
    "    # Set up the FSDP config. To enable FSDP via SPMD, set xla_fsdp_v2 to True.\n",
    "    fsdp_config = {\"fsdp_transformer_layer_cls_to_wrap\": [\n",
    "            \"LlamaDecoderLayer\"\n",
    "        ],\n",
    "        \"xla\": True,\n",
    "        \"xla_fsdp_v2\": True,\n",
    "        \"xla_fsdp_grad_ckpt\": True}\n",
    "\n",
    "    OUTPUT_DIR=f\"/tmp/{args.output_folder}\"\n",
    "    CHECKPOINT_DIR=f\"/tmp/{args.checkpoint_directory}\"\n",
    "\n",
    "    # Finally, set up the trainer and train the model.\n",
    "    trainer = SFTTrainer(\n",
    "        model=base_model,\n",
    "        train_dataset=data,\n",
    "        args=TrainingArguments(\n",
    "            per_device_train_batch_size=BATCH_SIZE,  # This is actually the global batch size for SPMD.\n",
    "            num_train_epochs=args.epochs,\n",
    "            max_steps=-1,\n",
    "            output_dir=OUTPUT_DIR,\n",
    "            optim=\"adafactor\",\n",
    "            logging_steps=1,\n",
    "            dataloader_drop_last = True,  # Required for SPMD.\n",
    "            fsdp=\"full_shard\",\n",
    "            fsdp_config=fsdp_config,\n",
    "        ),\n",
    "        peft_config=lora_config,\n",
    "        dataset_text_field=\"quote\",\n",
    "        max_seq_length=max_seq_length,\n",
    "        packing=True,\n",
    "    )\n",
    "\n",
    "    # train\n",
    "    trainer.train()\n",
    "    \n",
    "    adapter_model_id = \"adapter_model\"\n",
    "    adapter_path = f\"{CHECKPOINT_DIR}/{adapter_model_id}\"\n",
    "    merged_model_id = \"merged_model\"\n",
    "    merged_model_path = f\"{CHECKPOINT_DIR}/{merged_model_id}\"\n",
    "    \n",
    "    trainer.model.to('cpu').save_pretrained(adapter_path)\n",
    "    \n",
    "    # Save the adapter, merged model, and tokenizer\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(DL_DIR, torch_dtype=torch.bfloat16)\n",
    "    peft_model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "    merged_model = peft_model.merge_and_unload()\n",
    "    merged_model.save_pretrained(merged_model_path,safe_serialization=False)\n",
    "    tokenizer.save_pretrained(merged_model_path)\n",
    "    \n",
    "    # Copy merged files to GCS folder\n",
    "    OUTPUT_PREFIX=f\"{args.merged_model_folder}/{merged_model_id}/{xr.process_index()}/\"\n",
    "    upload_directory_with_transfer_manager(bucket_name=args.bucket_name,source_directory=merged_model_path,\n",
    "                                       blob_name_prefix=OUTPUT_PREFIX)\n",
    "    print ('Uploaded merged model files')\n",
    "\n",
    "    # copy adapter files to GCS folder\n",
    "    OUTPUT_PREFIX=f\"{args.merged_model_folder}/{adapter_model_id}/{xr.process_index()}/\"\n",
    "    upload_directory_with_transfer_manager(bucket_name=args.bucket_name,source_directory=adapter_path,\n",
    "                                       blob_name_prefix=OUTPUT_PREFIX)\n",
    "    print ('Uploaded adapter model files')\n",
    "\n",
    "    print ('Exiting job')\n",
    "    sys.exit(0)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pq4iF00YG_4T"
   },
   "source": [
    "## Fine-tune with Vertex AI Custom Training Jobs\n",
    "\n",
    "This section demonstrates how to fine-tune and deploy Llama2 models with PEFT LoRA on Vertex AI Custom Training Jobs. LoRA (Low-Rank Adaptation) is one approach of PEFT (Parameter Efficient Fine-tuning), where pretrained model weights are frozen and rank decomposition matrices representing the change in model weights are trained during fine-tuning. Read more about LoRA in the following publication: [Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L. and Chen, W., 2021. Lora: Low-rank adaptation of large language models. *arXiv preprint arXiv:2106.09685*](https://arxiv.org/abs/2106.09685)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1b90914fc81"
   },
   "source": [
    "#### Enable docker to run as a regular user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "232a259d3edc"
   },
   "outputs": [],
   "source": [
    "!sudo usermod -a -G docker ${USER}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e028d107fbe5"
   },
   "source": [
    "#### Change to the trainer directory to build the docker container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "3a9390f87b66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/vertex-ai-samples/notebooks/official/training/trainer2\n"
     ]
    }
   ],
   "source": [
    "%cd trainer2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81eb3c13afa9"
   },
   "source": [
    "#### Build the custom docker container and push to artifact registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-central1-docker.pkg.dev/ml-project-461521/tpuv5e-liama2-7b-training-repository-unique/llama2-7b-hf-lora-tuning-tpuv5e:latest\n"
     ]
    }
   ],
   "source": [
    "print(PYTORCH_TRAIN_DOCKER_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ee497559677f"
   },
   "outputs": [],
   "source": [
    "!docker build -t $PYTORCH_TRAIN_DOCKER_URI -f Dockerfile ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "0715b34162b4",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [us-central1-docker.pkg.dev/ml-project-461521/tpuv5e-liama2-7b-training-repository-unique/llama2-7b-hf-lora-tuning-tpuv5e]\n",
      "\n",
      "\u001b[1B60210300: Preparing \n",
      "\u001b[1B6225a757: Preparing \n",
      "\u001b[1B9cd0aaa8: Preparing \n",
      "\u001b[1B7bb0bd86: Preparing \n",
      "\u001b[1B3dace8df: Preparing \n",
      "\u001b[1B04911c6b: Preparing \n",
      "\u001b[1B0a3e5095: Preparing \n",
      "\u001b[1Bc513c5c5: Preparing \n",
      "\u001b[1Bfb0be0d3: Preparing \n",
      "\u001b[1B444d54a5: Preparing \n",
      "\u001b[1B81e633be: Preparing \n",
      "\u001b[1Be25316a9: Preparing \n",
      "\u001b[1B9a994928: Preparing \n",
      "\u001b[1B92eca7ee: Preparing \n",
      "\u001b[1B8db23091: Preparing \n",
      "\u001b[1B6141a482: Preparing \n",
      "\u001b[1Bf79174ac: Preparing \n",
      "\u001b[1Bb892285d: Preparing \n",
      "\u001b[14B4911c6b: Waiting g \n",
      "\u001b[14Ba3e5095: Waiting g \n",
      "\u001b[13Bb0be0d3: Waiting g \n",
      "\u001b[12B1e633be: Waiting g \n",
      "\u001b[12B25316a9: Waiting g \n",
      "\u001b[12Ba994928: Waiting g \n",
      "\u001b[10B141a482: Waiting g \n",
      "\u001b[9Bb892285d: Waiting g \n",
      "\u001b[11B79174ac: Waiting g \n",
      "\u001b[1B36fe7bf5: Preparing \n",
      "\u001b[7B81fcdce1: Waiting g \n",
      "\u001b[1Bebd30172: Preparing \n",
      "\u001b[8B07f0d13f: Waiting g \n",
      "\u001b[19B2eca7ee: Pushed   615.7MB/602.1MB-releases/docker/xla 2A\u001b[2K\u001b[30A\u001b[2K\u001b[26A\u001b[2K\u001b[27A\u001b[2K\u001b[26A\u001b[2K\u001b[27A\u001b[2K\u001b[26A\u001b[2K\u001b[25A\u001b[2K\u001b[27A\u001b[2K\u001b[25A\u001b[2K\u001b[31A\u001b[2K\u001b[25A\u001b[2K\u001b[23A\u001b[2K\u001b[31A\u001b[2K\u001b[25A\u001b[2K\u001b[22A\u001b[2K\u001b[31A\u001b[2K\u001b[25A\u001b[2K\u001b[24A\u001b[2K\u001b[25A\u001b[2K\u001b[24A\u001b[2K\u001b[25A\u001b[2K\u001b[31A\u001b[2K\u001b[25A\u001b[2K\u001b[31A\u001b[2K\u001b[23A\u001b[2K\u001b[31A\u001b[2K\u001b[23A\u001b[2K\u001b[31A\u001b[2K\u001b[23A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[23A\u001b[2K\u001b[31A\u001b[2K\u001b[23A\u001b[2K\u001b[21A\u001b[2K\u001b[23A\u001b[2K\u001b[21A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[21A\u001b[2K\u001b[23A\u001b[2K\u001b[21A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[23A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[21A\u001b[2K\u001b[19A\u001b[2K\u001b[23A\u001b[2K\u001b[24A\u001b[2K\u001b[23A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[23A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[23A\u001b[2K\u001b[23A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[23A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[18A\u001b[2K\u001b[19A\u001b[2K\u001b[31A\u001b[2K\u001b[23A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[23A\u001b[2K\u001b[19A\u001b[2K\u001b[23A\u001b[2K\u001b[17A\u001b[2K\u001b[19A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[23A\u001b[2K\u001b[24A\u001b[2K\u001b[23A\u001b[2K\u001b[19A\u001b[2K\u001b[23A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[23A\u001b[2K\u001b[31A\u001b[2K\u001b[23A\u001b[2K\u001b[19A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[23A\u001b[2K\u001b[24A\u001b[2K\u001b[23A\u001b[2K\u001b[24A\u001b[2K\u001b[23A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[23A\u001b[2K\u001b[19A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[23A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[23A\u001b[2K\u001b[31A\u001b[2K\u001b[23A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[23A\u001b[2K\u001b[24A\u001b[2K\u001b[23A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[31A\u001b[2K\u001b[9A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[6A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[5A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[4A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[3A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[24A\u001b[2K\u001b[24A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[24A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2Klatest: digest: sha256:93261e88dcb8a29442d76a9828193d4941dedb6d6d666bbb9bc6f22802a49523 size: 7061\n"
     ]
    }
   ],
   "source": [
    "!docker push $PYTORCH_TRAIN_DOCKER_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "586a13cb67b4"
   },
   "source": [
    "#### Change back to your home directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "937b7269c93b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/vertex-ai-samples/notebooks/official/training\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08bf867e8f4c"
   },
   "source": [
    "#### Set GCS folder locations and job configurations settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "17392b36e9c0"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HF_MODEL_ID' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 29\u001b[0m\n\u001b[1;32m     12\u001b[0m TPU_TOPOLOGY \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4x4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m DISPLAY_NAME_PREFIX \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama2-7b-lora-train-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTPU_TOPOLOGY\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m tpuv5e_llama2_peft_job \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisplay_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: get_job_name_with_datetime(DISPLAY_NAME_PREFIX),\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjob_spec\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworker_pool_specs\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m     19\u001b[0m             {\n\u001b[1;32m     20\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmachine_spec\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m     21\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmachine_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: MACHINE_TYPE,\n\u001b[1;32m     22\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtpu_topology\u001b[39m\u001b[38;5;124m\"\u001b[39m: TPU_TOPOLOGY,\n\u001b[1;32m     23\u001b[0m                 },\n\u001b[1;32m     24\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplica_count\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     25\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontainer_spec\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m     26\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_uri\u001b[39m\u001b[38;5;124m\"\u001b[39m: PYTORCH_TRAIN_DOCKER_URI,\n\u001b[1;32m     27\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m     28\u001b[0m                         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--tpu_topology=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTPU_TOPOLOGY\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m---> 29\u001b[0m                         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--model_name=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mHF_MODEL_ID\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     30\u001b[0m                         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--bucket_name=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBUCKET_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     31\u001b[0m                         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--output_folder=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOUTPUT_DIR_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     32\u001b[0m                         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--checkpoint_directory=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCHECKPOINT_DIR_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     33\u001b[0m                         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--epochs=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     34\u001b[0m                         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--merged_model_folder=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMERGED_MODEL_FOLDER\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     35\u001b[0m                     ],\n\u001b[1;32m     36\u001b[0m                 },\n\u001b[1;32m     37\u001b[0m             },\n\u001b[1;32m     38\u001b[0m         ],\n\u001b[1;32m     39\u001b[0m     },\n\u001b[1;32m     40\u001b[0m }\n\u001b[1;32m     42\u001b[0m tpuv5e_llama2_peft_job\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HF_MODEL_ID' is not defined"
     ]
    }
   ],
   "source": [
    "# Create a GCS folder to store the merged model with the base model and the\n",
    "# fine-tuned LORA adapter.\n",
    "BUCKET_NAME = BUCKET_URI.replace(\"gs://\", \"\")\n",
    "OUTPUT_DIR_NAME = \"output\"\n",
    "CHECKPOINT_DIR_NAME = \"output_chk\"\n",
    "NUM_EPOCHS = 200\n",
    "MERGED_MODEL_FOLDER = \"llama2-7b-hf/modelfiles\"\n",
    "\n",
    "# See machines type to match chips being used\n",
    "# Topologies of 2x2, 2x4, 4x4 = 4, 8, 16 chip settings and use quota from aiplatform.googleapis.com/custom_model_training_tpu_v5e\n",
    "MACHINE_TYPE = \"ct5lp-hightpu-4t\"\n",
    "TPU_TOPOLOGY = \"4x4\"\n",
    "\n",
    "DISPLAY_NAME_PREFIX = f\"llama2-7b-lora-train-{TPU_TOPOLOGY}\"\n",
    "tpuv5e_llama2_peft_job = {\n",
    "    \"display_name\": get_job_name_with_datetime(DISPLAY_NAME_PREFIX),\n",
    "    \"job_spec\": {\n",
    "        \"worker_pool_specs\": [\n",
    "            {\n",
    "                \"machine_spec\": {\n",
    "                    \"machine_type\": MACHINE_TYPE,\n",
    "                    \"tpu_topology\": TPU_TOPOLOGY,\n",
    "                },\n",
    "                \"replica_count\": 1,\n",
    "                \"container_spec\": {\n",
    "                    \"image_uri\": PYTORCH_TRAIN_DOCKER_URI,\n",
    "                    \"args\": [\n",
    "                        f\"--tpu_topology={TPU_TOPOLOGY}\",\n",
    "                        f\"--model_name={HF_MODEL_ID}\",\n",
    "                        f\"--bucket_name={BUCKET_NAME}\",\n",
    "                        f\"--output_folder={OUTPUT_DIR_NAME}\",\n",
    "                        f\"--checkpoint_directory={CHECKPOINT_DIR_NAME}\",\n",
    "                        f\"--epochs={NUM_EPOCHS}\",\n",
    "                        f\"--merged_model_folder={MERGED_MODEL_FOLDER}\",\n",
    "                    ],\n",
    "                },\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "}\n",
    "\n",
    "tpuv5e_llama2_peft_job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0c2bc267c4a5"
   },
   "source": [
    "#### Create job client and run job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "bc9b403c8515"
   },
   "outputs": [],
   "source": [
    "job_client = aiplatform.gapic.JobServiceClient(\n",
    "    client_options=dict(api_endpoint=f\"{LOCATION}-aiplatform.googleapis.com\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "4da63b786392",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"projects/190345877179/locations/us-central1/customJobs/8270430342762987520\"\n",
      "display_name: \"llama2-7b-lora-train-4x4_20250719_002245\"\n",
      "job_spec {\n",
      "  worker_pool_specs {\n",
      "    machine_spec {\n",
      "      machine_type: \"ct5lp-hightpu-4t\"\n",
      "      tpu_topology: \"4x4\"\n",
      "    }\n",
      "    replica_count: 1\n",
      "    disk_spec {\n",
      "      boot_disk_type: \"pd-ssd\"\n",
      "      boot_disk_size_gb: 100\n",
      "    }\n",
      "    container_spec {\n",
      "      image_uri: \"us-central1-docker.pkg.dev/ml-project-461521/tpuv5e-liama2-7b-training-repository-unique/llama2-7b-hf-lora-tuning-tpuv5e:latest\"\n",
      "      args: \"--tpu_topology=4x4\"\n",
      "      args: \"--model_name=meta-llama/Llama-2-7b-hf\"\n",
      "      args: \"--bucket_name=liama2-tpuv5e-ml-project-461521-unique\"\n",
      "      args: \"--output_folder=output\"\n",
      "      args: \"--checkpoint_directory=output_chk\"\n",
      "      args: \"--epochs=200\"\n",
      "      args: \"--merged_model_folder=llama2-7b-hf/modelfiles\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "state: JOB_STATE_PENDING\n",
      "create_time {\n",
      "  seconds: 1752884571\n",
      "  nanos: 83445000\n",
      "}\n",
      "update_time {\n",
      "  seconds: 1752884571\n",
      "  nanos: 83445000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "create_tpuv5e_llama2_peft_job_response = job_client.create_custom_job(\n",
    "    parent=\"projects/{project}/locations/{location}\".format(\n",
    "        project=PROJECT_ID, location=LOCATION\n",
    "    ),\n",
    "    custom_job=tpuv5e_llama2_peft_job,\n",
    ")\n",
    "print(create_tpuv5e_llama2_peft_job_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fb8dfcc23789"
   },
   "source": [
    "#### Check on job progress\n",
    "This may take 20-60 minutes or more depending on the model size. Run this cell multiple times to check progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "f402309d9dbb",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"projects/190345877179/locations/us-central1/customJobs/8270430342762987520\"\n",
       "display_name: \"llama2-7b-lora-train-4x4_20250719_002245\"\n",
       "job_spec {\n",
       "  worker_pool_specs {\n",
       "    machine_spec {\n",
       "      machine_type: \"ct5lp-hightpu-4t\"\n",
       "      tpu_topology: \"4x4\"\n",
       "    }\n",
       "    replica_count: 1\n",
       "    disk_spec {\n",
       "      boot_disk_type: \"pd-ssd\"\n",
       "      boot_disk_size_gb: 100\n",
       "    }\n",
       "    container_spec {\n",
       "      image_uri: \"us-central1-docker.pkg.dev/ml-project-461521/tpuv5e-liama2-7b-training-repository-unique/llama2-7b-hf-lora-tuning-tpuv5e:latest\"\n",
       "      args: \"--tpu_topology=4x4\"\n",
       "      args: \"--model_name=meta-llama/Llama-2-7b-hf\"\n",
       "      args: \"--bucket_name=liama2-tpuv5e-ml-project-461521-unique\"\n",
       "      args: \"--output_folder=output\"\n",
       "      args: \"--checkpoint_directory=output_chk\"\n",
       "      args: \"--epochs=200\"\n",
       "      args: \"--merged_model_folder=llama2-7b-hf/modelfiles\"\n",
       "    }\n",
       "  }\n",
       "}\n",
       "state: JOB_STATE_PENDING\n",
       "create_time {\n",
       "  seconds: 1752884571\n",
       "  nanos: 83445000\n",
       "}\n",
       "start_time {\n",
       "  seconds: 1752884571\n",
       "  nanos: 220646000\n",
       "}\n",
       "update_time {\n",
       "  seconds: 1752884573\n",
       "  nanos: 656077000\n",
       "}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tpuv5e_llama2_peft_job_response = job_client.get_custom_job(\n",
    "    name=create_tpuv5e_llama2_peft_job_response.name\n",
    ")\n",
    "get_tpuv5e_llama2_peft_job_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eb7f4f1ac160"
   },
   "source": [
    "#### Click on the console log url output from this cell to see your logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "babf40cf7821"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://console.cloud.google.com/logs/query;query=resource.labels.job_id=%228270430342762987520%22;cursorTimestamp=2025-07-19T02:46:56.905771Z;startTime=2025-07-18T00:22:56.905678Z;endTime=2025-07-19T02:46:56.905771Z?project=ml-project-461521\n"
     ]
    }
   ],
   "source": [
    "job_id = create_tpuv5e_llama2_peft_job_response.name[\n",
    "    create_tpuv5e_llama2_peft_job_response.name.rfind(\"/\") + 1 :\n",
    "]\n",
    "STARTDATE = datetime.today() - timedelta(days=1)\n",
    "STARTDATE = STARTDATE.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "ENDDATE = datetime.today() + timedelta(days=0.1)\n",
    "ENDDATE = ENDDATE.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "print(\n",
    "    f\"https://console.cloud.google.com/logs/query;query=resource.labels.job_id=%22{job_id}%22;cursorTimestamp={ENDDATE}Z;startTime={STARTDATE}Z;endTime={ENDDATE}Z?project={PROJECT_ID}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0f97d1b4fb05"
   },
   "source": [
    "#### Wait until the training job is complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "a6ecd909fea8",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is not complete and is in state JOB_STATE_PENDING\n",
      "Training is not complete and is in state JOB_STATE_PENDING\n",
      "Training is not complete and is in state JOB_STATE_PENDING\n",
      "Training is not complete and is in state JOB_STATE_PENDING\n",
      "Training is not complete and is in state JOB_STATE_PENDING\n",
      "Training is not complete and is in state JOB_STATE_PENDING\n",
      "Training is not complete and is in state JOB_STATE_RUNNING\n",
      "Training is not complete and is in state JOB_STATE_RUNNING\n",
      "Training is not complete and is in state JOB_STATE_RUNNING\n",
      "Training is not complete and is in state JOB_STATE_RUNNING\n",
      "Training is not complete and is in state JOB_STATE_RUNNING\n",
      "Training is not complete and is in state JOB_STATE_RUNNING\n",
      "Training is not complete and is in state JOB_STATE_RUNNING\n",
      "Training is not complete and is in state JOB_STATE_RUNNING\n",
      "Training is not complete and is in state JOB_STATE_RUNNING\n",
      "Training is not complete and is in state JOB_STATE_RUNNING\n",
      "Training is not complete and is in state JOB_STATE_RUNNING\n",
      "Training is not complete and is in state JOB_STATE_RUNNING\n",
      "Training is not complete and is in state JOB_STATE_RUNNING\n",
      "Training is not complete and is in state JOB_STATE_RUNNING\n",
      "Training is not complete and is in state JOB_STATE_RUNNING\n",
      "Training is not complete and is in state JOB_STATE_RUNNING\n",
      "Training is not complete and is in state JOB_STATE_RUNNING\n",
      "Training is not complete and is in state JOB_STATE_RUNNING\n",
      "Training is not complete and is in state JOB_STATE_RUNNING\n",
      "Training has completed\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "from google.cloud.aiplatform import gapic as aip\n",
    "\n",
    "while True:\n",
    "    response = job_client.get_custom_job(\n",
    "        name=create_tpuv5e_llama2_peft_job_response.name\n",
    "    )\n",
    "    if response.state != aip.JobState.JOB_STATE_SUCCEEDED:\n",
    "        print(f\"Training is not complete and is in state {response.state.name}\")\n",
    "        if response.state == aip.JobState.JOB_STATE_FAILED:\n",
    "            raise Exception(\"Training Job Failed\")\n",
    "    else:\n",
    "        print(\"Training has completed\")\n",
    "        break\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d52a6bb72aa6"
   },
   "source": [
    "### Deploy fine tuned models\n",
    "This section uploads the model to Model Registry and deploys the model using Hex-LLM, a High-Efficiency Large Language Model serving solution built with XLA that is being developed by Google Cloud\n",
    "\n",
    "The model deployment step will take 15-20 minutes to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "e1936eee4a7b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://liama2-tpuv5e-ml-project-461521-unique/llama2-7b-hf/modelfiles/merged_model/0'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HEXLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai-restricted/vertex-vision-model-garden-dockers/hex-llm-serve:20240328_RC01\"\n",
    "MERGED_MODEL_FOLDER = \"llama2-7b-hf/modelfiles\"\n",
    "\n",
    "# GCS folder path where the merged model files were saved in you bucket\n",
    "# MERGED_MODEL_FOLDER=\"llama2-7b-hf/modelfiles\" set during fine-tuning\n",
    "MERGED_MODEL_PATH = f\"{MERGED_MODEL_FOLDER}/merged_model/0\"\n",
    "GCS_MODEL_PATH = f\"{BUCKET_URI}/{MERGED_MODEL_PATH}\"\n",
    "\n",
    "DISPLAY_NAME_PREFIX = \"llama2-7b-lora-deploy\"  # @param {type:\"string\"}\n",
    "JOB_NAME = get_job_name_with_datetime(DISPLAY_NAME_PREFIX)\n",
    "GCS_MODEL_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c963a9dcf4ac"
   },
   "source": [
    "#### Check the model files in your GCS directory\n",
    "\n",
    "Your output should show a list of files like this\n",
    "```\n",
    "gs://<YOUR-BUCKET>/modelfiles/merged_model/config.json\n",
    "gs://<YOUR-BUCKET>/modelfiles/merged_model/generation_config.json\n",
    "gs://<YOUR-BUCKET>/modelfiles/merged_model/pytorch_model-00001-of-00003.bin\n",
    "gs://<YOUR-BUCKET>/modelfiles/merged_model/pytorch_model-00002-of-00003.bin\n",
    "gs://<YOUR-BUCKET>/modelfiles/merged_model/pytorch_model-00003-of-00003.bin\n",
    "gs://<YOUR-BUCKET>/modelfiles/merged_model/pytorch_model.bin.index.json\n",
    "gs://<YOUR-BUCKET>/modelfiles/merged_model/special_tokens_map.json\n",
    "gs://<YOUR-BUCKET>/modelfiles/merged_model/tokenizer.json\n",
    "gs://<YOUR-BUCKET>/modelfiles/merged_model/tokenizer_config.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "9159d64417a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://liama2-tpuv5e-ml-project-461521-unique/llama2-7b-hf/modelfiles/merged_model/0/config.json\n",
      "gs://liama2-tpuv5e-ml-project-461521-unique/llama2-7b-hf/modelfiles/merged_model/0/generation_config.json\n",
      "gs://liama2-tpuv5e-ml-project-461521-unique/llama2-7b-hf/modelfiles/merged_model/0/pytorch_model-00001-of-00003.bin\n",
      "gs://liama2-tpuv5e-ml-project-461521-unique/llama2-7b-hf/modelfiles/merged_model/0/pytorch_model-00002-of-00003.bin\n",
      "gs://liama2-tpuv5e-ml-project-461521-unique/llama2-7b-hf/modelfiles/merged_model/0/pytorch_model-00003-of-00003.bin\n",
      "gs://liama2-tpuv5e-ml-project-461521-unique/llama2-7b-hf/modelfiles/merged_model/0/pytorch_model.bin.index.json\n",
      "gs://liama2-tpuv5e-ml-project-461521-unique/llama2-7b-hf/modelfiles/merged_model/0/special_tokens_map.json\n",
      "gs://liama2-tpuv5e-ml-project-461521-unique/llama2-7b-hf/modelfiles/merged_model/0/tokenizer.json\n",
      "gs://liama2-tpuv5e-ml-project-461521-unique/llama2-7b-hf/modelfiles/merged_model/0/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls $GCS_MODEL_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0188453d4c9f"
   },
   "source": [
    "#### Define function for deploying model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "0931f14c09cb"
   },
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def deploy_model_hexllm(\n",
    "    model_name: str,\n",
    "    model_id: str,\n",
    "    service_account: str,\n",
    "    machine_type: str = \"ct5lp-hightpu-4t\",\n",
    "    max_num_batched_tokens: int = 11264,  # 11264\n",
    "    tokens_pad_multiple: int = 1024,\n",
    "    seqs_pad_multiple: int = 32,\n",
    "    sync: bool = True,\n",
    ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
    "    \"\"\"Deploys models with Hex-LLM on TPU in Vertex AI.\"\"\"\n",
    "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
    "\n",
    "    num_tpu_chips = int(machine_type[-2])\n",
    "    hexllm_args = [\n",
    "        \"--host=0.0.0.0\",\n",
    "        \"--port=7080\",\n",
    "        \"--log_level=INFO\",\n",
    "        f\"--model={model_id}\",\n",
    "        \"--load_format=pt\",  # Note: Using Pytorch bin format for weights\n",
    "        f\"--tensor_parallel_size={num_tpu_chips}\",\n",
    "        \"--num_nodes=1\",\n",
    "        \"--use_ray\",\n",
    "        \"--batch_mode=continuous\",\n",
    "        f\"--max_num_batched_tokens={max_num_batched_tokens}\",\n",
    "        f\"--tokens_pad_multiple={tokens_pad_multiple}\",\n",
    "        f\"--seqs_pad_multiple={seqs_pad_multiple}\",\n",
    "    ]\n",
    "\n",
    "    env_vars = {\n",
    "        \"PJRT_DEVICE\": \"TPU\",\n",
    "        \"RAY_DEDUP_LOGS\": \"0\",\n",
    "        \"RAY_USAGE_STATS_ENABLED\": \"0\",\n",
    "    }\n",
    "\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=model_name,\n",
    "        serving_container_image_uri=HEXLLM_DOCKER_URI,\n",
    "        serving_container_command=[\"python\", \"-m\", \"hex_llm.entrypoints.api_server\"],\n",
    "        serving_container_args=hexllm_args,\n",
    "        serving_container_ports=[7080],\n",
    "        serving_container_predict_route=\"/generate\",\n",
    "        serving_container_health_route=\"/ping\",\n",
    "        serving_container_environment_variables=env_vars,\n",
    "        serving_container_shared_memory_size_mb=(16 * 1024),  # 16 GB\n",
    "        serving_container_deployment_timeout=7200,\n",
    "    )\n",
    "\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        machine_type=machine_type,\n",
    "        deploy_request_timeout=1800,\n",
    "        service_account=service_account,\n",
    "        sync=sync,\n",
    "    )\n",
    "    return model, endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "\n",
    "def deploy_model_hexllm_gpu(\n",
    "    model_name: str,\n",
    "    model_id: str,\n",
    "    service_account: str,\n",
    "    machine_type: str = \"n1-standard-8\",\n",
    "    accelerator_type: str = \"NVIDIA_TESLA_T4\",\n",
    "    accelerator_count: int = 1,\n",
    "    max_num_batched_tokens: int = 4096,\n",
    "    tokens_pad_multiple: int = 512,\n",
    "    seqs_pad_multiple: int = 16,\n",
    "    sync: bool = True,\n",
    ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
    "    \"\"\"Deploys Hex-LLM model on GPU in Vertex AI.\"\"\"\n",
    "\n",
    "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
    "\n",
    "    hexllm_args = [\n",
    "        \"--host=0.0.0.0\",\n",
    "        \"--port=7080\",\n",
    "        \"--log_level=INFO\",\n",
    "        f\"--model={model_id}\",\n",
    "        \"--load_format=pt\",\n",
    "        \"--tensor_parallel_size=1\",  # GPU: 1 chip\n",
    "        \"--num_nodes=1\",\n",
    "        \"--use_ray\",\n",
    "        \"--batch_mode=continuous\",\n",
    "        f\"--max_num_batched_tokens={max_num_batched_tokens}\",\n",
    "        f\"--tokens_pad_multiple={tokens_pad_multiple}\",\n",
    "        f\"--seqs_pad_multiple={seqs_pad_multiple}\",\n",
    "    ]\n",
    "\n",
    "    env_vars = {\n",
    "        \"PJRT_DEVICE\": \"CUDA\",  # указывает на использование GPU\n",
    "        \"RAY_DEDUP_LOGS\": \"0\",\n",
    "        \"RAY_USAGE_STATS_ENABLED\": \"0\",\n",
    "    }\n",
    "\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=model_name,\n",
    "        serving_container_image_uri=HEXLLM_DOCKER_URI,\n",
    "        serving_container_command=[\"python\", \"-m\", \"hex_llm.entrypoints.api_server\"],\n",
    "        serving_container_args=hexllm_args,\n",
    "        serving_container_ports=[7080],\n",
    "        serving_container_predict_route=\"/generate\",\n",
    "        serving_container_health_route=\"/ping\",\n",
    "        serving_container_environment_variables=env_vars,\n",
    "        serving_container_shared_memory_size_mb=(8 * 1024),  # 8 GB\n",
    "        serving_container_deployment_timeout=3600,\n",
    "    )\n",
    "\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "        deploy_request_timeout=1800,\n",
    "        service_account=service_account,\n",
    "        sync=sync,\n",
    "    )\n",
    "\n",
    "    return model, endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8945e2e6d6ed"
   },
   "source": [
    "#### Deploy model to Vertex\n",
    "The `deploy_model_hexllm` function will return a reference to the model added to the Vertex AI Model Registry as well as a new endpoint where the model will be deployed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ecd851116c14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model from:  gs://liama2-tpuv5e-ml-project-461521-unique/llama2-7b-hf/modelfiles/merged_model/0\n",
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/190345877179/locations/us-central1/endpoints/2204156875342086144/operations/2588088434337775616\n",
      "Endpoint created. Resource name: projects/190345877179/locations/us-central1/endpoints/2204156875342086144\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/190345877179/locations/us-central1/endpoints/2204156875342086144')\n",
      "Creating Model\n",
      "Create Model backing LRO: projects/190345877179/locations/us-central1/models/2268075884311216128/operations/5873464362504552448\n",
      "Model created. Resource name: projects/190345877179/locations/us-central1/models/2268075884311216128@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/190345877179/locations/us-central1/models/2268075884311216128@1')\n",
      "Deploying model to Endpoint : projects/190345877179/locations/us-central1/endpoints/2204156875342086144\n",
      "endpoint_name: 2204156875342086144\n",
      "Deploy Endpoint model backing LRO: projects/190345877179/locations/us-central1/endpoints/2204156875342086144/operations/8372962155695177728\n",
      "Deploying model to Endpoint : projects/190345877179/locations/us-central1/endpoints/5186665728568197120\n",
      "Deploy Endpoint model backing LRO: projects/190345877179/locations/us-central1/endpoints/5186665728568197120/operations/3092491592603271168\n"
     ]
    }
   ],
   "source": [
    "print(\"Using model from: \", GCS_MODEL_PATH)\n",
    "model, endpoint = deploy_model_hexllm(\n",
    "    model_name=JOB_NAME,\n",
    "    model_id=GCS_MODEL_PATH,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    sync=False,\n",
    ")\n",
    "print(\"endpoint_name:\", endpoint.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model from:  gs://liama2-tpuv5e-ml-project-461521-unique/llama2-7b-hf/modelfiles/merged_model/0\n",
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/190345877179/locations/us-central1/endpoints/5186665728568197120/operations/1367612935320371200\n",
      "Endpoint created. Resource name: projects/190345877179/locations/us-central1/endpoints/5186665728568197120\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/190345877179/locations/us-central1/endpoints/5186665728568197120')\n",
      "Creating Model\n",
      "Create Model backing LRO: projects/190345877179/locations/us-central1/models/1741154727908868096/operations/835062279383810048\n",
      "Model created. Resource name: projects/190345877179/locations/us-central1/models/1741154727908868096@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/190345877179/locations/us-central1/models/1741154727908868096@1')\n",
      "endpoint_name: 5186665728568197120\n"
     ]
    }
   ],
   "source": [
    "print(\"Using model from: \", GCS_MODEL_PATH)\n",
    "model, endpoint = deploy_model_hexllm_gpu(\n",
    "    model_name=JOB_NAME,\n",
    "    model_id=GCS_MODEL_PATH,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    sync=False,\n",
    ")\n",
    "print(\"endpoint_name:\", endpoint.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81650c5e3444"
   },
   "source": [
    "#### Review the logs after the model has been deployed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "7e31488fc924"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://console.cloud.google.com/logs/query;query=resource.type%3D%22aiplatform.googleapis.com%2FEndpoint%22%20resource.labels.endpoint_id%3D%225186665728568197120%22%20resource.labels.location%3D%22us-central1%22;startTime=2025-07-18T15:59:28.466543Z;endTime=2025-07-19T18:23:28.466637Z?project=ml-project-461521\n"
     ]
    }
   ],
   "source": [
    "ENDPOINT_ID = endpoint.name[endpoint.name.rfind(\"/\") + 1 :]\n",
    "STARTDATE = datetime.today() - timedelta(days=1)\n",
    "STARTDATE = STARTDATE.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "ENDDATE = datetime.today() + timedelta(days=0.1)\n",
    "ENDDATE = ENDDATE.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "print(\n",
    "    f\"https://console.cloud.google.com/logs/query;query=resource.type%3D%22aiplatform.googleapis.com%2FEndpoint%22%20resource.labels.endpoint_id%3D%22{ENDPOINT_ID}%22%20resource.labels.location%3D%22{LOCATION}%22;startTime={STARTDATE}Z;endTime={ENDDATE}Z?project={PROJECT_ID}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adb2ed90a241"
   },
   "source": [
    "#### Wait until endpoint is complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "4c92f6cdd08d"
   },
   "outputs": [
    {
     "ename": "FailedPrecondition",
     "evalue": "400 Model server exited unexpectedly. Model server logs can be found at https://console.cloud.google.com/logs/viewer?project=190345877179&resource=aiplatform.googleapis.com%2FEndpoint&advancedFilter=resource.type%3D%22aiplatform.googleapis.com%2FEndpoint%22%0Aresource.labels.endpoint_id%3D%225186665728568197120%22%0Aresource.labels.location%3D%22us-central1%22. 9: Model server exited unexpectedly. Model server logs can be found at https://console.cloud.google.com/logs/viewer?project=190345877179&resource=aiplatform.googleapis.com%2FEndpoint&advancedFilter=resource.type%3D%22aiplatform.googleapis.com%2FEndpoint%22%0Aresource.labels.endpoint_id%3D%225186665728568197120%22%0Aresource.labels.location%3D%22us-central1%22.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPrecondition\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform/base.py:306\u001b[0m, in \u001b[0;36mFutureManager.wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m future:\n\u001b[1;32m    304\u001b[0m     futures\u001b[38;5;241m.\u001b[39mwait([future], return_when\u001b[38;5;241m=\u001b[39mfutures\u001b[38;5;241m.\u001b[39mFIRST_EXCEPTION)\n\u001b[0;32m--> 306\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_future_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform/base.py:272\u001b[0m, in \u001b[0;36mFutureManager._raise_future_exception\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__latest_future_lock:\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m--> 272\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform/base.py:284\u001b[0m, in \u001b[0;36mFutureManager._complete_future\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__latest_future_lock:\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m         \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# raises\u001b[39;00m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m e\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform/base.py:374\u001b[0m, in \u001b[0;36mFutureManager._submit.<locals>.wait_for_dependencies_and_invoke\u001b[0;34m(deps, method, args, kwargs, internal_callbacks)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(deps):\n\u001b[1;32m    372\u001b[0m     future\u001b[38;5;241m.\u001b[39mresult()\n\u001b[0;32m--> 374\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call callbacks from within future\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m internal_callbacks:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform/models.py:6097\u001b[0m, in \u001b[0;36mModel._deploy\u001b[0;34m(self, endpoint, deployed_model_display_name, traffic_percentage, traffic_split, machine_type, min_replica_count, max_replica_count, accelerator_type, accelerator_count, tpu_topology, reservation_affinity_type, reservation_affinity_key, reservation_affinity_values, service_account, explanation_spec, metadata, encryption_spec_key_name, network, sync, deploy_request_timeout, autoscaling_target_cpu_utilization, autoscaling_target_accelerator_duty_cycle, autoscaling_target_request_count_per_minute, spot, enable_access_logging, disable_container_logging, private_service_connect_config, deployment_resource_pool, fast_tryout_enabled, system_labels, required_replica_count)\u001b[0m\n\u001b[1;32m   6085\u001b[0m         endpoint \u001b[38;5;241m=\u001b[39m PrivateEndpoint\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m   6086\u001b[0m             display_name\u001b[38;5;241m=\u001b[39mdisplay_name,\n\u001b[1;32m   6087\u001b[0m             network\u001b[38;5;241m=\u001b[39mnetwork,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6092\u001b[0m             private_service_connect_config\u001b[38;5;241m=\u001b[39mprivate_service_connect_config,\n\u001b[1;32m   6093\u001b[0m         )\n\u001b[1;32m   6095\u001b[0m _LOGGER\u001b[38;5;241m.\u001b[39mlog_action_start_against_resource(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeploying model to\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, endpoint)\n\u001b[0;32m-> 6097\u001b[0m \u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_deploy_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   6098\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6099\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresource_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6100\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gca_resource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraffic_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeployed_model_display_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeployed_model_display_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraffic_percentage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraffic_percentage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraffic_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraffic_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmachine_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmachine_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_replica_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_replica_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_replica_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_replica_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6109\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccelerator_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6110\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccelerator_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtpu_topology\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtpu_topology\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreservation_affinity_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreservation_affinity_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreservation_affinity_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreservation_affinity_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreservation_affinity_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreservation_affinity_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice_account\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_account\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexplanation_spec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexplanation_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeploy_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeploy_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mautoscaling_target_cpu_utilization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoscaling_target_cpu_utilization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mautoscaling_target_accelerator_duty_cycle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoscaling_target_accelerator_duty_cycle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mautoscaling_target_request_count_per_minute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoscaling_target_request_count_per_minute\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6123\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_access_logging\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_access_logging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_container_logging\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_container_logging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeployment_resource_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeployment_resource_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfast_tryout_enabled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfast_tryout_enabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6127\u001b[0m \u001b[43m    \u001b[49m\u001b[43msystem_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msystem_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequired_replica_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequired_replica_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6129\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6131\u001b[0m _LOGGER\u001b[38;5;241m.\u001b[39mlog_action_completed_against_resource(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeployed\u001b[39m\u001b[38;5;124m\"\u001b[39m, endpoint)\n\u001b[1;32m   6133\u001b[0m endpoint\u001b[38;5;241m.\u001b[39m_sync_gca_resource()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform/models.py:2147\u001b[0m, in \u001b[0;36mEndpoint._deploy_call\u001b[0;34m(cls, api_client, endpoint_resource_name, model, endpoint_resource_traffic_split, network, deployed_model_display_name, traffic_percentage, traffic_split, machine_type, min_replica_count, max_replica_count, accelerator_type, accelerator_count, tpu_topology, reservation_affinity_type, reservation_affinity_key, reservation_affinity_values, service_account, explanation_spec, metadata, deploy_request_timeout, autoscaling_target_cpu_utilization, autoscaling_target_accelerator_duty_cycle, autoscaling_target_request_count_per_minute, spot, enable_access_logging, disable_container_logging, deployment_resource_pool, fast_tryout_enabled, system_labels, required_replica_count)\u001b[0m\n\u001b[1;32m   2135\u001b[0m operation_future \u001b[38;5;241m=\u001b[39m api_client\u001b[38;5;241m.\u001b[39mdeploy_model(\n\u001b[1;32m   2136\u001b[0m     endpoint\u001b[38;5;241m=\u001b[39mendpoint_resource_name,\n\u001b[1;32m   2137\u001b[0m     deployed_model\u001b[38;5;241m=\u001b[39mdeployed_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2140\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mdeploy_request_timeout,\n\u001b[1;32m   2141\u001b[0m )\n\u001b[1;32m   2143\u001b[0m _LOGGER\u001b[38;5;241m.\u001b[39mlog_action_started_against_resource_with_lro(\n\u001b[1;32m   2144\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeploy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mcls\u001b[39m, operation_future\n\u001b[1;32m   2145\u001b[0m )\n\u001b[0;32m-> 2147\u001b[0m \u001b[43moperation_future\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/api_core/future/polling.py:261\u001b[0m, in \u001b[0;36mPollingFuture.result\u001b[0;34m(self, timeout, retry, polling)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking_poll(timeout\u001b[38;5;241m=\u001b[39mtimeout, retry\u001b[38;5;241m=\u001b[39mretry, polling\u001b[38;5;241m=\u001b[39mpolling)\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# pylint: disable=raising-bad-type\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# Pylint doesn't recognize that this is valid in this case.\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "\u001b[0;31mFailedPrecondition\u001b[0m: 400 Model server exited unexpectedly. Model server logs can be found at https://console.cloud.google.com/logs/viewer?project=190345877179&resource=aiplatform.googleapis.com%2FEndpoint&advancedFilter=resource.type%3D%22aiplatform.googleapis.com%2FEndpoint%22%0Aresource.labels.endpoint_id%3D%225186665728568197120%22%0Aresource.labels.location%3D%22us-central1%22. 9: Model server exited unexpectedly. Model server logs can be found at https://console.cloud.google.com/logs/viewer?project=190345877179&resource=aiplatform.googleapis.com%2FEndpoint&advancedFilter=resource.type%3D%22aiplatform.googleapis.com%2FEndpoint%22%0Aresource.labels.endpoint_id%3D%225186665728568197120%22%0Aresource.labels.location%3D%22us-central1%22."
     ]
    }
   ],
   "source": [
    "endpoint.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3d894b10257a"
   },
   "outputs": [],
   "source": [
    "# (optional) Wait 15 minutes while the model is downloaded and setup\n",
    "if os.getenv(\"IS_TESTING\"):\n",
    "    time.sleep(900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "221a5b47cd7d"
   },
   "source": [
    "NOTE: The overall deployment can take 30-40 minutes or more. After the deployment succeeds (15-20 minutes or so), the fine-tuned model will be downloaded from the GCS bucket used in training above. Thus, an additional ~15-20 minutes (depending on the model sizes) of waiting time is needed **after** the model deployment step above succeeds and before you run the next step below. Otherwise you might see a `ServiceUnavailable: 503 502:Bad Gateway` error when you send requests to the endpoint.\n",
    "\n",
    "### Once deployment is ready, send a prediction request\n",
    "\n",
    "Once deployment succeeds, you can send requests to the endpoint with text prompts. The first request will take a minute or two while model warmup occurs\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "Prompt: Provide a list of the 3 best comedy movies in the 90s in 50 characters or less\n",
    "Response:  1) The Cable Guy 2) Scooby-Doo 3) Beethoven Requirements\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0a85ec25e11f"
   },
   "outputs": [],
   "source": [
    "PROMPT = (\n",
    "    \"Provide a list of the 3 best comedy movies in the 90s in 50 characters or less\"\n",
    ")\n",
    "\n",
    "instances = [\n",
    "    {\n",
    "        \"prompt\": PROMPT,\n",
    "        \"max_tokens\": 80,\n",
    "        \"temperature\": 1.0,\n",
    "        \"top_p\": 1.0,\n",
    "        \"top_k\": 1.0,\n",
    "    },\n",
    "]\n",
    "\n",
    "response = endpoint.predict(instances=instances)\n",
    "\n",
    "for prediction in response.predictions:\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "af21a3cff1e0"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "911406c1561e"
   },
   "outputs": [],
   "source": [
    "# Delete the train job.\n",
    "job_client.delete_custom_job(name=create_tpuv5e_llama2_peft_job_response.name)\n",
    "\n",
    "# Undeploy model and delete endpoint.\n",
    "endpoint.delete(force=True)\n",
    "\n",
    "# Delete models.\n",
    "model.delete()\n",
    "\n",
    "import os\n",
    "\n",
    "# Delete Cloud Storage objects that were created\n",
    "delete_bucket = False\n",
    "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil -m rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tpuv5e_llama2_pytorch_finetuning_and_serving.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m131",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
