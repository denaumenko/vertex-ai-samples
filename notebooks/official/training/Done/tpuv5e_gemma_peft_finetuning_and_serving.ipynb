{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Vertex AI - Gemma distributed tuning with LoRA on TPUv5e, serving on L4 GPU\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/training/tpuv5e_gemma_peft_finetuning_and_serving.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"><br> Run in Colab (Will require the higher memory Colab pro)\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fofficial%2Ftraining%2Ftpuv5e_gemma_peft_finetuning_and_serving.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://cloud.google.com/ml-engine/images/colab-enterprise-logo-32px.png\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/notebooks/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/training/tpuv5e_gemma_peft_finetuning_and_serving.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br>\n",
    "Open in Vertex AI Workbench\n",
    "    </a> (An e2-standard-8 CPU w/ 250GB disk is recommended)\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/training/tpuv5e_gemma_peft_finetuning_and_serving.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br>\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook is based on the [LoRA tuning example on ai.google.dev](https://ai.google.dev/gemma/docs/distributed_tuning). It follows an existing [Model Garden example written for fine-tuning on GPUs](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_gemma_finetuning_on_vertex.ipynb), and has been modified to use the latest TPUv5e chips for training. It demonstrates fine-tuning and deploying Gemma models with [Vertex AI Custom Training Job](https://cloud.google.com/vertex-ai/docs/training/create-custom-job). A Vertex AI Custom Training Job allows for a higher level of customization and control over the fine-tuning job. All of the examples in this notebook use parameter efficient fine-tuning methods [PEFT](https://github.com/huggingface/peft) to reduce training and storage costs.\n",
    "\n",
    "This notebook deploys the model with a [vLLM](https://github.com/vllm-project/vllm) docker\n",
    "\n",
    "\n",
    "### Objective\n",
    "\n",
    "- Fine-tune and deploy Gemma models with a Vertex AI Custom Training Job.\n",
    "- Send prediction requests to your fine-tuned Gemma model.\n",
    "\n",
    "\n",
    "### Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08d289fa873f"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "In this example, the IMDB reviews dataset from TensorFlow datasets is used to finetune the model. Details of the dataset can be found here: https://www.tensorflow.org/datasets/catalog/imdb_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aed92deeb4a0"
   },
   "source": [
    "### Costs \n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "Vertex AI (Training, TPUv5e, L4 GPU), Cloud Storage\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing), [Cloud NL API pricing](https://cloud.google.com/natural-language/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "## Get started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81711614c199"
   },
   "source": [
    "### Install Vertex AI SDK for Python and other required packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2b4ef9b72d43"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# (optional) update gcloud if needed\n",
    "if os.getenv(\"IS_TESTING\"):\n",
    "    ! gcloud components update --quiet\n",
    "\n",
    "! pip3 install --upgrade --quiet google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58707a750154"
   },
   "source": [
    "### Restart runtime (Colab only)\n",
    "\n",
    "To use the newly installed packages, you must restart the runtime on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "f200f10a1da3"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffcde4d56c00"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7176ea64999b"
   },
   "source": [
    "### Authenticate your notebook environment (Colab only)\n",
    "\n",
    "Authenticate your environment on Google Colab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "7de6ef0fac42"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "909aa61d1f13"
   },
   "source": [
    "### Kaggle credentials\n",
    "Gemma models are hosted by Kaggle. To use Gemma, request access on Kaggle:\n",
    "\n",
    "* Sign in or register at [kaggle.com](https://www.kaggle.com)\n",
    "* Open the [Gemma model card](https://www.kaggle.com/models/google/gemma) and select \"Request Access\"\n",
    "* Complete the consent form and accept the terms and conditions\n",
    "\n",
    "Then, to use the Kaggle API, create an API token:\n",
    "\n",
    "* Open [Kaggle settings](https://www.kaggle.com/settings)\n",
    "* Select \"Create New Token\"\n",
    "* A kaggle.json file is downloaded. It contains your Kaggle credentials. Take note of the username and key since you'll use these later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4f872cd812d0"
   },
   "source": [
    "### Set Google Cloud project information and initialize Vertex AI SDK for Python\n",
    "\n",
    "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "#### Location\n",
    "\n",
    "You can also change the `LOCATION` variable used by Vertex AI. Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations).\n",
    "\n",
    "TPUv5e is available in the [following regions listed here](https://cloud.google.com/tpu/pricing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "region"
   },
   "outputs": [],
   "source": [
    "LOCATION = \"us-central1\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "oM1iC_MfAts1"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"ml-project-461521\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "960505627ddf"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "Create a storage bucket to store intermediate artifacts such as datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = f\"gs://tpu-training-gemma-7b-{PROJECT_ID}-unique\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "648eb19b83dd"
   },
   "source": [
    "#### Set folder paths for staging, environment, and model artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "855d6b96f291"
   },
   "outputs": [],
   "source": [
    "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
    "MODEL_BUCKET = os.path.join(BUCKET_URI, \"model\")\n",
    "\n",
    "# The service account looks like:\n",
    "# '@.iam.gserviceaccount.com'\n",
    "# Please go to https://cloud.google.com/iam/docs/service-accounts-create#iam-service-accounts-create-console\n",
    "# and create service account with `Vertex AI User` and `Storage Object Admin` roles.\n",
    "# The service account for deploying fine tuned model.\n",
    "SERVICE_ACCOUNT = \"190345877179-compute@developer.gserviceaccount.com\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EcIXiGsCePi"
   },
   "source": [
    "**If your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://tpu-training-gemma-7b-ml-project-461521-unique/...\n"
     ]
    }
   ],
   "source": [
    "! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "source": [
    "### Initialize Vertex AI SDK for Python\n",
    "\n",
    "Initialize the Vertex AI SDK for Python for your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=LOCATION, staging_bucket=STAGING_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7c13158fa7be"
   },
   "source": [
    "### Select the Gemma base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "d698e6e0c126"
   },
   "outputs": [],
   "source": [
    "# The Gemma base model.\n",
    "base_model = \"google/gemma-7b-it\"  # @param [\"google/gemma-2b\", \"google/gemma-2b-it\", \"google/gemma-7b\", \"google/gemma-7b-it\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "832bbd8cb9ef"
   },
   "source": [
    "### Create the artifact registry repository and set the custom docker image uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "18fd49b88127"
   },
   "outputs": [],
   "source": [
    "REPOSITORY = \"tpuv5e-training-repository-unique-7b-it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "c2242d87effa"
   },
   "outputs": [],
   "source": [
    "image_name_train = \"gemma-lora-tuning-tpuv5e\"\n",
    "hostname = f\"{LOCATION}-docker.pkg.dev\"\n",
    "tag = \"latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "9dd94acb26e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mWARNING:\u001b[0m Your config file at [/home/jupyter/.docker/config.json] contains these credential helper entries:\n",
      "\n",
      "{\n",
      "  \"credHelpers\": {\n",
      "    \"gcr.io\": \"gcloud\",\n",
      "    \"us.gcr.io\": \"gcloud\",\n",
      "    \"eu.gcr.io\": \"gcloud\",\n",
      "    \"asia.gcr.io\": \"gcloud\",\n",
      "    \"staging-k8s.gcr.io\": \"gcloud\",\n",
      "    \"marketplace.gcr.io\": \"gcloud\",\n",
      "    \"us-west1-docker.pkg.dev\": \"gcloud\",\n",
      "    \"us-central1-docker.pkg.dev\": \"gcloud\"\n",
      "  }\n",
      "}\n",
      "Adding credentials for: us-central1-docker.pkg.dev\n",
      "gcloud credential helpers already registered correctly.\n"
     ]
    }
   ],
   "source": [
    "# Register gcloud as a Docker credential helper\n",
    "!gcloud auth configure-docker $LOCATION-docker.pkg.dev --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "5c8e8364a097"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mERROR:\u001b[0m (gcloud.artifacts.repositories.create) ALREADY_EXISTS: the repository already exists\n"
     ]
    }
   ],
   "source": [
    "# One time or use an existing repository\n",
    "!gcloud artifacts repositories create $REPOSITORY --repository-format=docker \\\n",
    "--location=$LOCATION --description=\"Vertex TPUv5e training repository\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "16488f1fc702"
   },
   "outputs": [],
   "source": [
    "# Define container image name\n",
    "KERAS_TRAIN_DOCKER_URI = (\n",
    "    f\"{hostname}/{PROJECT_ID}/{REPOSITORY}/{image_name_train}:{tag}\"\n",
    ")\n",
    "\n",
    "# Set the docker image uri for the vLLM serving container\n",
    "VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20240220_0936_RC01\"\n",
    "\n",
    "# Set the docker image uri for the model conversion container that converts the fine-tuned model to HF format\n",
    "KERAS_MODEL_CONVERSION_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/jax-keras-model-conversion:20240220_0936_RC01\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0c250872074f"
   },
   "source": [
    "### Define common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "354da31189dc"
   },
   "outputs": [],
   "source": [
    "def get_job_name_with_datetime(prefix: str) -> str:\n",
    "    \"\"\"Gets the job name with date time when triggering training or deployment\n",
    "    jobs in Vertex AI.\n",
    "    \"\"\"\n",
    "    return prefix + datetime.now().strftime(\"_%Y%m%d_%H%M%S\")\n",
    "\n",
    "\n",
    "def deploy_model_vllm(\n",
    "    model_name: str,\n",
    "    model_uri: str,\n",
    "    service_account: str,\n",
    "    machine_type: str = \"g2-standard-8\",\n",
    "    accelerator_type: str = \"NVIDIA_L4\",\n",
    "    accelerator_count: int = 1,\n",
    "    max_model_len: int = 8192,\n",
    "    dtype: str = \"bfloat16\",\n",
    ") -> tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
    "    # Upload the model to \"Model Registry\"\n",
    "    job_name = get_job_name_with_datetime(model_name)\n",
    "    vllm_args = [\n",
    "        \"--host=0.0.0.0\",\n",
    "        \"--port=7080\",\n",
    "        f\"--tensor-parallel-size={accelerator_count}\",\n",
    "        \"--swap-space=16\",\n",
    "        \"--gpu-memory-utilization=0.95\",\n",
    "        f\"--max-model-len={max_model_len}\",\n",
    "        f\"--dtype={dtype}\",\n",
    "        \"--disable-log-stats\",\n",
    "    ]\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=job_name,\n",
    "        artifact_uri=model_uri,\n",
    "        serving_container_image_uri=VLLM_DOCKER_URI,\n",
    "        serving_container_command=[\"python\", \"-m\", \"vllm.entrypoints.api_server\"],\n",
    "        serving_container_args=vllm_args,\n",
    "        serving_container_ports=[7080],\n",
    "        serving_container_predict_route=\"/generate\",\n",
    "        serving_container_health_route=\"/ping\",\n",
    "    )\n",
    "\n",
    "    # Deploy the model to an endpoint to serve \"Online predictions\"\n",
    "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "        deploy_request_timeout=1800,\n",
    "        service_account=service_account,\n",
    "        sync=False,\n",
    "    )\n",
    "\n",
    "    return model, endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e11e1f8a6c4c"
   },
   "source": [
    "### Build the Docker container files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19d584f9fe62"
   },
   "source": [
    "#### Create the trainer directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "909e93e7fd43"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(\"trainer\"):\n",
    "    os.makedirs(\"trainer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c725bbcabe27"
   },
   "source": [
    "#### Kaggle credentials are required for KerasNLP training and Hex-LLM deployment with TPUs.\n",
    "Set the KAGGLE_USERNAME AND KAGGLE_KEY to pass in as an environment variable for Vertex Training to use\n",
    "Fenerate the Kaggle username and key by following [these instructions](https://github.com/Kaggle/kaggle-api?tab=readme-ov-file#api-credentials).\n",
    "As mentioned earlier, you need to review and accept the model license."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "0480a14d88e3"
   },
   "outputs": [],
   "source": [
    "KAGGLE_USERNAME = \"denaumenko1\"  # @param {type:\"string\", isTemplate:true}\n",
    "KAGGLE_KEY = \"7616f231472466443ca6a69f6a5a9012\"  # @param {type:\"string\", isTemplate:true}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8a91ddf0cbcb"
   },
   "source": [
    "#### Create the Dockerfile for the custom container. This will install JAX[TPU], Keras, and TensorFlow datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "756577886992"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting trainer/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile trainer/Dockerfile\n",
    "# This Dockerfile fine tunes the Gemma model using LoRA with JAX\n",
    "\n",
    "FROM python:3.10\n",
    "\n",
    "ENV DEBIAN_FRONTEND=noninteractive\n",
    "\n",
    "# Install basic libs\n",
    "RUN apt-get update && apt-get -y upgrade && apt-get install -y --no-install-recommends \\\n",
    "        cmake \\\n",
    "        curl \\\n",
    "        wget \\\n",
    "        sudo \\\n",
    "        gnupg \\\n",
    "        libsm6 \\\n",
    "        libxext6 \\\n",
    "        libxrender-dev \\\n",
    "        lsb-release \\\n",
    "        ca-certificates \\\n",
    "        build-essential \\\n",
    "        git \\\n",
    "        libgl1\n",
    "\n",
    "# Copy Apache license.\n",
    "RUN wget https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/LICENSE\n",
    "\n",
    "# Install required libs\n",
    "RUN pip install --upgrade pip\n",
    "RUN pip install jax[tpu]==0.4.25 -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
    "RUN pip install tensorflow==2.15.0.post1\n",
    "RUN pip install tensorflow-datasets==4.9.4\n",
    "RUN pip install -q -U keras-nlp==0.8.2\n",
    "RUN pip install keras==3.0.5\n",
    "\n",
    "# Copy other licenses.\n",
    "RUN wget -O MIT_LICENSE https://github.com/pytest-dev/pytest/blob/main/LICENSE\n",
    "RUN wget -O BSD_LICENSE https://github.com/pytorch/xla/blob/master/LICENSE\n",
    "RUN wget -O BSD-3_LICENSE https://github.com/pytorch/pytorch/blob/main/LICENSE\n",
    "\n",
    "ENV KERAS_BACKEND=jax\n",
    "ENV XLA_PYTHON_CLIENT_MEM_FRACTION=0.9\n",
    "ENV TPU_LIBRARY_PATH=/lib/libtpu.so\n",
    "\n",
    "# Copy install libtpu to PATH above\n",
    "RUN find ./usr/local/lib -name 'libtpu.so' -exec cp {} /lib \\;\n",
    "\n",
    "WORKDIR /\n",
    "COPY train.py train.py\n",
    "ENV PYTHONPATH ./\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ee7ac9469ce"
   },
   "source": [
    "#### Add the __init__.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "b0b57ecd02d8"
   },
   "outputs": [],
   "source": [
    "!touch trainer/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c648296e1057"
   },
   "source": [
    "#### Add the train.py file\n",
    "This code is from the LoRA distributed fine-tuning code from this example: https://ai.google.dev/gemma/docs/distributed_tuning\n",
    "\n",
    "The IMDB TensorFlow dataset is used to fine-tune the Gemma model. Additional logic is added to handle the TPU topology setting required by TPUv5e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "1b6a47a6089c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting trainer/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile trainer/train.py\n",
    "import os\n",
    "import argparse\n",
    "import shutil\n",
    "import locale\n",
    "\n",
    "# Переменные для сохранения модели и токенизатора\n",
    "_ENCODING_FOR_MODEL_SAVING = \"UTF-8\"\n",
    "_VOCABULARY_FILENAME = \"vocabulary.spm\"\n",
    "_TOKENIZER_FILENAME = \"tokenizer.model\"\n",
    "\n",
    "# Импорт библиотек Keras, TensorFlow, KerasNLP и TensorFlow Datasets\n",
    "import keras\n",
    "import keras_nlp\n",
    "import tensorflow\n",
    "import tensorflow_datasets as tfds\n",
    "print (keras.__version__)\n",
    "print (tensorflow.__version__)\n",
    "\n",
    "# Параметры, передаваемые в скрипт\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--tpu_topology\",\n",
    "    help=\"Топология TPUv5e (например, 1x1, 2x2 и т.д.)\",\n",
    "    type=str\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--model_name\",\n",
    "    help=\"Имя модели Gemma (например, gemma_2b_en)\",\n",
    "    type=str\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--output_folder\",\n",
    "    type=str,\n",
    "    required=True,\n",
    "    help=\"Путь к директории, куда сохранить чекпоинт и токенизатор\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--checkpoint_filename\",\n",
    "    type=str,\n",
    "    default=\"fine_tuned.weights.h5\",\n",
    "    help=\"Имя файла для сохранения чекпоинта\",\n",
    ")\n",
    "args = parser.parse_args()\n",
    "\n",
    "def main():\n",
    "    # Парсинг топологии TPU (например, 2x2)\n",
    "    x = args.tpu_topology.split(\"x\")\n",
    "    tpu_topology_x = int(x[0])\n",
    "    tpu_topology_y = int(x[1])\n",
    "    print (f'TPU topology is ({tpu_topology_x}, {tpu_topology_y})')\n",
    "    print (f'Model name is {args.model_name}')\n",
    "\n",
    "    # Создание сетки устройств (mesh) для распределённого обучения\n",
    "    device_mesh = keras.distribution.DeviceMesh(\n",
    "        (tpu_topology_x, tpu_topology_y),\n",
    "        [\"batch\", \"model\"],\n",
    "        devices=keras.distribution.list_devices())\n",
    "\n",
    "    model_dim = \"model\"\n",
    "\n",
    "    # Настройка шардинга весов модели\n",
    "    layout_map = keras.distribution.LayoutMap(device_mesh)\n",
    "    layout_map[\"token_embedding/embeddings\"] = (None, model_dim)\n",
    "    layout_map[\"decoder_block.*attention.*(query|key|value).*kernel\"] = (\n",
    "        None, model_dim, None)\n",
    "    layout_map[\"decoder_block.*attention_output.*kernel\"] = (\n",
    "        None, None, model_dim)\n",
    "    layout_map[\"decoder_block.*ffw_gating.*kernel\"] = (model_dim, None)\n",
    "    layout_map[\"decoder_block.*ffw_linear.*kernel\"] = (None, model_dim)\n",
    "\n",
    "    # Применение стратегии параллелизма модели\n",
    "    model_parallel = keras.distribution.ModelParallel(\n",
    "        device_mesh, layout_map, batch_dim_name=\"batch\")\n",
    "    keras.distribution.set_distribution(model_parallel)\n",
    "\n",
    "    # Загрузка модели Gemma по имени\n",
    "    model_name = args.model_name\n",
    "    gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(args.model_name)\n",
    "\n",
    "    # Инференс до fine-tuning\n",
    "    print (f'Running inference on the base {args.model_name} model')\n",
    "    lm_output = gemma_lm.generate(\"Prompt: Return 3 things I ask for in this format. \\\n",
    "        Response: 1) item 1 2) item 2 3) item 3. \\\n",
    "        Prompt: List the 3 best comedy movies in the 90s Response: \", max_length=100)\n",
    "    print (lm_output)\n",
    "\n",
    "    # Загрузка датасета IMDB\n",
    "    imdb_train = tfds.load(\n",
    "        \"imdb_reviews\",\n",
    "        split=\"train\",\n",
    "        as_supervised=True,\n",
    "        batch_size=2,\n",
    "    )\n",
    "    # Удаление меток (оставляем только текст)\n",
    "    imdb_train = imdb_train.map(lambda x, y: x)\n",
    "\n",
    "    # Просмотр одного примера\n",
    "    imdb_train.unbatch().take(1).get_single_element().numpy()\n",
    "\n",
    "    # Включение LoRA в backbone модели\n",
    "    gemma_lm.backbone.enable_lora(rank=4)\n",
    "\n",
    "    # Ограничение длины последовательности\n",
    "    gemma_lm.preprocessor.sequence_length = 128\n",
    "\n",
    "    # Настройка оптимизатора AdamW\n",
    "    optimizer = keras.optimizers.AdamW(\n",
    "        learning_rate=5e-5,\n",
    "        weight_decay=0.01,\n",
    "    )\n",
    "    # Исключение из weight decay: bias и scale\n",
    "    optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n",
    "\n",
    "    # Компиляция модели с функцией потерь и метриками\n",
    "    gemma_lm.compile(\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer=optimizer,\n",
    "        weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "    gemma_lm.summary()\n",
    "\n",
    "    # Обучение модели на 1 эпоху\n",
    "    gemma_lm.fit(imdb_train, epochs=1)\n",
    "\n",
    "    # Инференс после fine-tuning\n",
    "    print (f'Running inference on the fine-tuned {args.model_name} model')\n",
    "    lm_output = gemma_lm.generate(\"Prompt: Return 3 things I ask for in this format. \\\n",
    "        Response: 1) item 1 2) item 2 3) item 3. \\\n",
    "        Prompt: List the 3 best comedy movies in the 90s Response: \", max_length=100)\n",
    "    print (lm_output) \n",
    "\n",
    "    # Сохранение чекпоинта и токенизатора\n",
    "    print(\"Saving checkpoint and tokenizer.\")\n",
    "    if not os.path.exists(args.output_folder):\n",
    "        os.makedirs(args.output_folder)\n",
    "    locale.getpreferredencoding = lambda: _ENCODING_FOR_MODEL_SAVING\n",
    "    gemma_lm.save_weights(\n",
    "        os.path.join(args.output_folder, args.checkpoint_filename)\n",
    "    )\n",
    "    gemma_lm.preprocessor.tokenizer.save_assets(args.output_folder)\n",
    "\n",
    "    # Копирование и переименование файла токенизатора\n",
    "    print(\"Copying tokenizer file.\")\n",
    "    shutil.copy(\n",
    "        os.path.join(args.output_folder, _VOCABULARY_FILENAME),\n",
    "        os.path.join(args.output_folder, _TOKENIZER_FILENAME),\n",
    "    )\n",
    "    print ('Exiting job')\n",
    "\n",
    "# Точка входа в скрипт\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pq4iF00YG_4T"
   },
   "source": [
    "## Fine-tune with Vertex AI Custom Training Jobs\n",
    "\n",
    "This section demonstrates how to fine-tune and deploy Gemma models with PEFT LoRA on Vertex AI Custom Training Jobs. LoRA (Low-Rank Adaptation) is one approach of PEFT (Parameter Efficient Fine-tuning), where pretrained model weights are frozen and rank decomposition matrices representing the change in model weights are trained during fine-tuning. Read more about LoRA in the following publication: [Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L. and Chen, W., 2021. Lora: Low-rank adaptation of large language models. *arXiv preprint arXiv:2106.09685*](https://arxiv.org/abs/2106.09685)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1b90914fc81"
   },
   "source": [
    "#### Enable docker to run as a regular user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "232a259d3edc"
   },
   "outputs": [],
   "source": [
    "!sudo usermod -a -G docker ${USER}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e028d107fbe5"
   },
   "source": [
    "#### Change to the trainer directory to build the docker container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "3a9390f87b66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/vertex-ai-samples/notebooks/official/training/trainer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81eb3c13afa9"
   },
   "source": [
    "#### Build the custom docker container and push to artifact registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-central1-docker.pkg.dev/ml-project-461521/tpuv5e-training-repository-unique-7b-it/gemma-lora-tuning-tpuv5e:latest\n"
     ]
    }
   ],
   "source": [
    "print(KERAS_TRAIN_DOCKER_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "ee497559677f",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Building 0.0s (0/1)                                          docker:default\n",
      "\u001b[?25h\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.2s (1/2)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.48kB                                     0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.10             0.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.3s (1/2)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.48kB                                     0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.10             0.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.4s (2/2)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.48kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/python:3.10             0.4s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.6s (19/19) FINISHED                               docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.48kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/python:3.10             0.4s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 5.75kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/15] FROM docker.io/library/python:3.10@sha256:875c3591e586f66aa65  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/15] RUN apt-get update && apt-get -y upgrade && apt-get in  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/15] RUN wget https://raw.githubusercontent.com/GoogleCloud  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/15] RUN pip install --upgrade pip                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/15] RUN pip install jax[tpu]==0.4.25 -f https://storage.go  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/15] RUN pip install tensorflow==2.15.0.post1                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 7/15] RUN pip install tensorflow-datasets==4.9.4              0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 8/15] RUN pip install -q -U keras-nlp==0.8.2                  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 9/15] RUN pip install keras==3.0.5                            0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [10/15] RUN wget -O MIT_LICENSE https://github.com/pytest-dev/  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [11/15] RUN wget -O BSD_LICENSE https://github.com/pytorch/xla  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [12/15] RUN wget -O BSD-3_LICENSE https://github.com/pytorch/p  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [13/15] RUN find ./usr/local/lib -name 'libtpu.so' -exec cp {}  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [14/15] COPY train.py train.py                                  0.0s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     0.0s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    0.0s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:dd7078c27ef7d7582bd3a76c99be9149dcd93016d6ba3  0.0s\n",
      "\u001b[0m\u001b[34m => => naming to us-central1-docker.pkg.dev/ml-project-461521/tpuv5e-trai  0.0s\n",
      "\u001b[0m\u001b[?25h\n",
      " \u001b[33m1 warning found (use docker --debug to expand):\n",
      "\u001b[0m - LegacyKeyValueFormat: \"ENV key=value\" should be used instead of legacy \"ENV key value\" format (line 48)\n"
     ]
    }
   ],
   "source": [
    "!docker build -t $KERAS_TRAIN_DOCKER_URI -f Dockerfile ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "0715b34162b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [us-central1-docker.pkg.dev/ml-project-461521/tpuv5e-training-repository-unique-7b-it/gemma-lora-tuning-tpuv5e]\n",
      "\n",
      "\u001b[1B8f67f029: Preparing \n",
      "\u001b[1B37694c0f: Preparing \n",
      "\u001b[1Ba2369350: Preparing \n",
      "\u001b[1Bb63ba4ac: Preparing \n",
      "\u001b[1B73d2e4ea: Preparing \n",
      "\u001b[1B2407f1d4: Preparing \n",
      "\u001b[1Bf342cfa2: Preparing \n",
      "\u001b[1Bc9a2c33e: Preparing \n",
      "\u001b[1B6a54940c: Preparing \n",
      "\u001b[1Be1bf17a1: Preparing \n",
      "\u001b[1B6a682e1e: Preparing \n",
      "\u001b[1Bcb67400a: Preparing \n",
      "\u001b[1Bdd27167e: Preparing \n",
      "\u001b[1B408139bb: Preparing \n",
      "\u001b[1B51729d89: Preparing \n",
      "\u001b[1B95464665: Preparing \n",
      "\u001b[1B3e410343: Preparing \n",
      "\u001b[1B05ffff1c: Preparing \n",
      "\u001b[1Bd283e64a: Preparing \n",
      "\u001b[2Bd283e64a: Mounted from ml-project-461521/tpuv5e-training-repository-unique/gemma-lora-tuning-tpuv5e \u001b[17A\u001b[2K\u001b[20A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[12A\u001b[2K\u001b[11A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2Klatest: digest: sha256:1cb83be733f6c83b892f6559fef5b5156f3d359caf5faae948cc028e40c84f55 size: 4540\n"
     ]
    }
   ],
   "source": [
    "!docker push $KERAS_TRAIN_DOCKER_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "586a13cb67b4"
   },
   "source": [
    "#### Change back to your home directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "937b7269c93b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/vertex-ai-samples/notebooks/official/training\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08bf867e8f4c"
   },
   "source": [
    "#### Set GCS folder locations and job configurations settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "17392b36e9c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'display_name': 'gemma-lora-train_20250718_203547',\n",
       " 'job_spec': {'worker_pool_specs': [{'machine_spec': {'machine_type': 'ct5lp-hightpu-4t',\n",
       "     'tpu_topology': '2x4'},\n",
       "    'replica_count': 1,\n",
       "    'container_spec': {'image_uri': 'us-central1-docker.pkg.dev/ml-project-461521/tpuv5e-training-repository-unique-7b-it/gemma-lora-tuning-tpuv5e:latest',\n",
       "     'args': ['--tpu_topology=2x4',\n",
       "      '--model_name=gemma_instruct_7b_en',\n",
       "      '--output_folder=/gcs/tpu-training-gemma-7b-ml-project-461521-unique/model/gemma-lora-model-tpuv5_20250718_203547',\n",
       "      '--checkpoint_filename=fine_tuned.weights.h5'],\n",
       "     'env': [{'name': 'KAGGLE_USERNAME', 'value': 'denaumenko1'},\n",
       "      {'name': 'KAGGLE_KEY', 'value': '7616f231472466443ca6a69f6a5a9012'}]}}]}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a GCS folder to store the merged model with the base model and the\n",
    "# fine-tuned LORA adapter.\n",
    "merged_model_dir = get_job_name_with_datetime(\"gemma-lora-model-tpuv5\")\n",
    "merged_model_output_dir = os.path.join(MODEL_BUCKET, merged_model_dir)\n",
    "merged_model_output_dir_gcsfuse = merged_model_output_dir.replace(\"gs://\", \"/gcs/\")\n",
    "\n",
    "# Set the checkpoint output filename\n",
    "checkpoint_filename = \"fine_tuned.weights.h5\"\n",
    "\n",
    "DISPLAY_NAME_PREFIX = \"gemma-lora-train\"  # @param {type:\"string\"}\n",
    "tpuv5e_gemma_peft_job = {\n",
    "    \"display_name\": get_job_name_with_datetime(DISPLAY_NAME_PREFIX),\n",
    "    \"job_spec\": {\n",
    "        \"worker_pool_specs\": [\n",
    "            {\n",
    "                \"machine_spec\": {\n",
    "                    \"machine_type\": \"ct5lp-hightpu-4t\",\n",
    "                    \"tpu_topology\": \"2x4\",\n",
    "                },\n",
    "                \"replica_count\": 1,\n",
    "                \"container_spec\": {\n",
    "                    \"image_uri\": KERAS_TRAIN_DOCKER_URI,\n",
    "                    \"args\": [\n",
    "                        \"--tpu_topology=2x4\",\n",
    "                        \"--model_name=gemma_instruct_7b_en\",\n",
    "                        f\"--output_folder={merged_model_output_dir_gcsfuse}\",\n",
    "                        f\"--checkpoint_filename={checkpoint_filename}\",\n",
    "                    ],\n",
    "                    \"env\": [\n",
    "                        {\"name\": \"KAGGLE_USERNAME\", \"value\": KAGGLE_USERNAME},\n",
    "                        {\"name\": \"KAGGLE_KEY\", \"value\": KAGGLE_KEY},\n",
    "                    ],\n",
    "                },\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "}\n",
    "\n",
    "tpuv5e_gemma_peft_job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0c2bc267c4a5"
   },
   "source": [
    "#### Create job client and run job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "bc9b403c8515"
   },
   "outputs": [],
   "source": [
    "job_client = aiplatform.gapic.JobServiceClient(\n",
    "    client_options=dict(api_endpoint=f\"{LOCATION}-aiplatform.googleapis.com\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "4da63b786392",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "ResourceExhausted",
     "evalue": "429 The following quota metrics exceed quota limits: aiplatform.googleapis.com/custom_model_training_tpu_v5e",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/api_core/grpc_helpers.py:76\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/grpc/_interceptor.py:277\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    270\u001b[0m     request: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    275\u001b[0m     compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    276\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 277\u001b[0m     response, ignored_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/grpc/_interceptor.py:332\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._with_call\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    329\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interceptor\u001b[38;5;241m.\u001b[39mintercept_unary_unary(\n\u001b[1;32m    330\u001b[0m     continuation, client_call_details, request\n\u001b[1;32m    331\u001b[0m )\n\u001b[0;32m--> 332\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, call\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/grpc/_channel.py:440\u001b[0m, in \u001b[0;36m_InactiveRpcError.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"See grpc.Future.result.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 440\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/grpc/_interceptor.py:315\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._with_call.<locals>.continuation\u001b[0;34m(new_details, request)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 315\u001b[0m     response, call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_thunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_method\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_credentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_wait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_compression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _UnaryOutcome(response, call)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/grpc/_channel.py:1198\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.with_call\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1192\u001b[0m (\n\u001b[1;32m   1193\u001b[0m     state,\n\u001b[1;32m   1194\u001b[0m     call,\n\u001b[1;32m   1195\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(\n\u001b[1;32m   1196\u001b[0m     request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[1;32m   1197\u001b[0m )\n\u001b[0;32m-> 1198\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/grpc/_channel.py:1006\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1006\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.RESOURCE_EXHAUSTED\n\tdetails = \"The following quota metrics exceed quota limits: aiplatform.googleapis.com/custom_model_training_tpu_v5e\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:74.125.202.95:443 {grpc_status:8, grpc_message:\"The following quota metrics exceed quota limits: aiplatform.googleapis.com/custom_model_training_tpu_v5e\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mResourceExhausted\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m create_tpuv5e_gemma_peft_job_response \u001b[38;5;241m=\u001b[39m \u001b[43mjob_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_custom_job\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprojects/\u001b[39;49m\u001b[38;5;132;43;01m{project}\u001b[39;49;00m\u001b[38;5;124;43m/locations/\u001b[39;49m\u001b[38;5;132;43;01m{location}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPROJECT_ID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLOCATION\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_job\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtpuv5e_gemma_peft_job\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(create_tpuv5e_gemma_peft_job_response)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform_v1/services/job_service/client.py:1284\u001b[0m, in \u001b[0;36mJobServiceClient.create_custom_job\u001b[0;34m(self, request, parent, custom_job, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m-> 1284\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1289\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/api_core/gapic_v1/method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/api_core/grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mResourceExhausted\u001b[0m: 429 The following quota metrics exceed quota limits: aiplatform.googleapis.com/custom_model_training_tpu_v5e"
     ]
    }
   ],
   "source": [
    "create_tpuv5e_gemma_peft_job_response = job_client.create_custom_job(\n",
    "    parent=\"projects/{project}/locations/{location}\".format(\n",
    "        project=PROJECT_ID, location=LOCATION\n",
    "    ),\n",
    "    custom_job=tpuv5e_gemma_peft_job,\n",
    ")\n",
    "print(create_tpuv5e_gemma_peft_job_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fb8dfcc23789"
   },
   "source": [
    "#### Check on job progress\n",
    "This may take 20-60 minutes or more depending on the model size. Run this cell multiple times to check progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "f402309d9dbb",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"projects/190345877179/locations/us-central1/customJobs/8142851809568882688\"\n",
       "display_name: \"gemma-lora-train_20250718_202105\"\n",
       "job_spec {\n",
       "  worker_pool_specs {\n",
       "    machine_spec {\n",
       "      machine_type: \"ct5lp-hightpu-4t\"\n",
       "      tpu_topology: \"2x2\"\n",
       "    }\n",
       "    replica_count: 1\n",
       "    disk_spec {\n",
       "      boot_disk_type: \"pd-ssd\"\n",
       "      boot_disk_size_gb: 100\n",
       "    }\n",
       "    container_spec {\n",
       "      image_uri: \"us-central1-docker.pkg.dev/ml-project-461521/tpuv5e-training-repository-unique-7b-it/gemma-lora-tuning-tpuv5e:latest\"\n",
       "      args: \"--tpu_topology=2x2\"\n",
       "      args: \"--model_name=gemma_instruct_7b_en\"\n",
       "      args: \"--output_folder=/gcs/tpu-training-gemma-7b-ml-project-461521-unique/model/gemma-lora-model-tpuv5_20250718_202105\"\n",
       "      args: \"--checkpoint_filename=fine_tuned.weights.h5\"\n",
       "      env {\n",
       "        name: \"KAGGLE_USERNAME\"\n",
       "        value: \"denaumenko1\"\n",
       "      }\n",
       "      env {\n",
       "        name: \"KAGGLE_KEY\"\n",
       "        value: \"7616f231472466443ca6a69f6a5a9012\"\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "state: JOB_STATE_PENDING\n",
       "create_time {\n",
       "  seconds: 1752870071\n",
       "  nanos: 538961000\n",
       "}\n",
       "start_time {\n",
       "  seconds: 1752870071\n",
       "  nanos: 684528000\n",
       "}\n",
       "update_time {\n",
       "  seconds: 1752870074\n",
       "  nanos: 242816000\n",
       "}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tpuv5e_gemma_peft_job_response = job_client.get_custom_job(\n",
    "    name=create_tpuv5e_gemma_peft_job_response.name\n",
    ")\n",
    "get_tpuv5e_gemma_peft_job_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eb7f4f1ac160"
   },
   "source": [
    "#### Click on the console log url output from this cell to see your logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-18 20:21:22.603996: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752870082.835189    7109 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752870082.903301    7109 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752870083.454677    7109 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752870083.454726    7109 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752870083.454729    7109 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752870083.454732    7109 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-18 20:21:23.514122: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['gemma_2b_en', 'gemma_instruct_2b_en', 'gemma_1.1_instruct_2b_en', 'code_gemma_1.1_2b_en', 'code_gemma_2b_en', 'gemma_7b_en', 'gemma_instruct_7b_en', 'gemma_1.1_instruct_7b_en', 'code_gemma_7b_en', 'code_gemma_instruct_7b_en', 'code_gemma_1.1_instruct_7b_en', 'gemma2_2b_en', 'gemma2_instruct_2b_en', 'gemma2_9b_en', 'gemma2_instruct_9b_en', 'gemma2_27b_en', 'gemma2_instruct_27b_en', 'shieldgemma_2b_en', 'shieldgemma_9b_en', 'shieldgemma_27b_en'])\n"
     ]
    }
   ],
   "source": [
    "from keras_nlp.models import GemmaCausalLM\n",
    "print(GemmaCausalLM.presets.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "babf40cf7821"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://console.cloud.google.com/logs/query;query=resource.labels.job_id=%228142851809568882688%22%20timestamp%3E=2025-07-17\n"
     ]
    }
   ],
   "source": [
    "job_id = create_tpuv5e_gemma_peft_job_response.name[\n",
    "    create_tpuv5e_gemma_peft_job_response.name.rfind(\"/\") + 1 :\n",
    "]\n",
    "startdate = datetime.today() - timedelta(days=1)\n",
    "startdate = startdate.strftime(\"%Y-%m-%d\")\n",
    "print(\n",
    "    f\"https://console.cloud.google.com/logs/query;query=resource.labels.job_id=%22{job_id}%22%20timestamp%3E={startdate}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0a1e0dbdf24"
   },
   "source": [
    "### Convert the fine-tuned Keras checkpoint to HF format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ae10c82ba26e"
   },
   "source": [
    "#### Download the conversion script from KerasNLP tools\n",
    "The GitHub repo is https://github.com/keras-team/keras-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "b02a8f0a54ab"
   },
   "outputs": [],
   "source": [
    "!wget -nv -nc https://raw.githubusercontent.com/keras-team/keras-nlp/master/tools/gemma/export_gemma_to_hf.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c77580797b26"
   },
   "source": [
    "#### Download the fine-tuned checkpoint files locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://tpu-training-gemma-7b-ml-project-461521-unique/model/gemma-lora-model-tpuv5_20250718_154802\n"
     ]
    }
   ],
   "source": [
    "print(merged_model_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "ba0e4080beba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://tpu-training-gemma-ml-project-461521-unique/model/gemma-lora-model-tpuv5_20250717_212334/fine_tuned.weights.h5 to file://./gemma-lora-model-tpuv5_20250717_212334/fine_tuned.weights.h5\n",
      "Copying gs://tpu-training-gemma-ml-project-461521-unique/model/gemma-lora-model-tpuv5_20250717_212334/tokenizer.model to file://./gemma-lora-model-tpuv5_20250717_212334/tokenizer.model\n",
      "Copying gs://tpu-training-gemma-ml-project-461521-unique/model/gemma-lora-model-tpuv5_20250717_212334/vocabulary.spm to file://./gemma-lora-model-tpuv5_20250717_212334/vocabulary.spm\n",
      "  Completed files 3/3 | 9.4GiB/9.4GiB | 195.9MiB/s                             \n",
      "\n",
      "Average throughput: 246.7MiB/s\n"
     ]
    }
   ],
   "source": [
    "!gcloud storage cp -r $merged_model_output_dir ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52c738bc53fc"
   },
   "source": [
    "#### Install libraries for model conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "dc8e6fc5327e",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.1\n",
      "  Using cached torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch==2.1) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch==2.1) (4.14.1)\n",
      "Collecting sympy (from torch==2.1)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch==2.1) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch==2.1) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch==2.1) (2025.5.1)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.1)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.1)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.1)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.1)\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.1)\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.1)\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.1)\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.1)\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.1)\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.18.1 (from torch==2.1)\n",
      "  Using cached nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.1)\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.1.0 (from torch==2.1)\n",
      "  Using cached triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from jinja2->torch==2.1) (3.0.2)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch==2.1)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Using cached torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
      "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Using cached nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Using cached triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "\u001b[2K  Attempting uninstall: torch━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m14/16\u001b[0m [nvidia-cusolver-cu12]2]\n",
      "\u001b[2K    Found existing installation: torch 1.13.190m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m14/16\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K    Uninstalling torch-1.13.1:━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m15/16\u001b[0m [torch]olver-cu12]\n",
      "\u001b[2K      Successfully uninstalled torch-1.13.10m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m15/16\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m15/16\u001b[0m [torch]\u001b[33m  WARNING: Failed to remove contents in a temporary directory '/opt/conda/envs/pytorch/lib/python3.10/site-packages/~orch'.\n",
      "  You can safely remove it manually.\u001b[0m\u001b[33m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/16\u001b[0m [torch]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.14.1 requires torch==1.13.1, but you have torch 2.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed mpmath-1.3.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 sympy-1.14.0 torch-2.1.0 triton-2.1.0\n",
      "Collecting keras-nlp\n",
      "  Using cached keras_nlp-0.21.1-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting keras-hub==0.21.1 (from keras-nlp)\n",
      "  Using cached keras_hub-0.21.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting keras>=3.5 (from keras-hub==0.21.1->keras-nlp)\n",
      "  Using cached keras-3.10.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: absl-py in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from keras-hub==0.21.1->keras-nlp) (2.3.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from keras-hub==0.21.1->keras-nlp) (2.1.3)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from keras-hub==0.21.1->keras-nlp) (25.0)\n",
      "Collecting regex (from keras-hub==0.21.1->keras-nlp)\n",
      "  Using cached regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: rich in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from keras-hub==0.21.1->keras-nlp) (13.9.4)\n",
      "Collecting kagglehub (from keras-hub==0.21.1->keras-nlp)\n",
      "  Using cached kagglehub-0.3.12-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting tensorflow-text (from keras-hub==0.21.1->keras-nlp)\n",
      "  Using cached tensorflow_text-2.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting namex (from keras>=3.5->keras-hub==0.21.1->keras-nlp)\n",
      "  Using cached namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting h5py (from keras>=3.5->keras-hub==0.21.1->keras-nlp)\n",
      "  Using cached h5py-3.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting optree (from keras>=3.5->keras-hub==0.21.1->keras-nlp)\n",
      "  Using cached optree-0.16.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (30 kB)\n",
      "Collecting ml-dtypes (from keras>=3.5->keras-hub==0.21.1->keras-nlp)\n",
      "  Using cached ml_dtypes-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from kagglehub->keras-hub==0.21.1->keras-nlp) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from kagglehub->keras-hub==0.21.1->keras-nlp) (2.32.4)\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from kagglehub->keras-hub==0.21.1->keras-nlp) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from optree->keras>=3.5->keras-hub==0.21.1->keras-nlp) (4.14.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->kagglehub->keras-hub==0.21.1->keras-nlp) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->kagglehub->keras-hub==0.21.1->keras-nlp) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->kagglehub->keras-hub==0.21.1->keras-nlp) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->kagglehub->keras-hub==0.21.1->keras-nlp) (2025.6.15)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from rich->keras-hub==0.21.1->keras-nlp) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from rich->keras-hub==0.21.1->keras-nlp) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras-hub==0.21.1->keras-nlp) (0.1.2)\n",
      "Collecting tensorflow<2.20,>=2.19.0 (from tensorflow-text->keras-hub==0.21.1->keras-nlp)\n",
      "  Using cached tensorflow-2.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.21.1->keras-nlp)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.21.1->keras-nlp)\n",
      "  Using cached flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.21.1->keras-nlp)\n",
      "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.21.1->keras-nlp)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.21.1->keras-nlp)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.21.1->keras-nlp)\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.21.1->keras-nlp) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.21.1->keras-nlp) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.21.1->keras-nlp) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.21.1->keras-nlp)\n",
      "  Using cached termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.21.1->keras-nlp) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.21.1->keras-nlp) (1.73.1)\n",
      "Collecting tensorboard~=2.19.0 (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.21.1->keras-nlp)\n",
      "  Using cached tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.21.1->keras-nlp)\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.21.1->keras-nlp)\n",
      "  Using cached markdown-3.8.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.21.1->keras-nlp)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.21.1->keras-nlp)\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.21.1->keras-nlp) (0.45.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19.0->tensorflow-text->keras-hub==0.21.1->keras-nlp) (3.0.2)\n",
      "Using cached keras_nlp-0.21.1-py3-none-any.whl (1.8 kB)\n",
      "Using cached keras_hub-0.21.1-py3-none-any.whl (876 kB)\n",
      "Using cached keras-3.10.0-py3-none-any.whl (1.4 MB)\n",
      "Using cached h5py-3.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "Using cached kagglehub-0.3.12-py3-none-any.whl (67 kB)\n",
      "Using cached ml_dtypes-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "Using cached namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Using cached optree-0.16.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (405 kB)\n",
      "Using cached regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "Using cached tensorflow_text-2.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
      "Using cached tensorflow-2.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.8 MB)\n",
      "Using cached tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "Using cached markdown-3.8.2-py3-none-any.whl (106 kB)\n",
      "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Using cached tensorflow_io_gcs_filesystem-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "Using cached termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, regex, optree, opt-einsum, ml-dtypes, markdown, h5py, google-pasta, gast, astunparse, tensorboard, kagglehub, keras, tensorflow, tensorflow-text, keras-hub, keras-nlp\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23/23\u001b[0m [keras-nlp]23\u001b[0m [keras-hub]-text]-server]stem]\n",
      "\u001b[1A\u001b[2KSuccessfully installed astunparse-1.6.3 flatbuffers-25.2.10 gast-0.6.0 google-pasta-0.2.0 h5py-3.14.0 kagglehub-0.3.12 keras-3.10.0 keras-hub-0.21.1 keras-nlp-0.21.1 libclang-18.1.1 markdown-3.8.2 ml-dtypes-0.5.1 namex-0.1.0 opt-einsum-3.4.0 optree-0.16.0 regex-2024.11.6 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 tensorflow-io-gcs-filesystem-0.37.1 tensorflow-text-2.19.0 termcolor-3.1.0 werkzeug-3.1.3\n",
      "Collecting accelerate\n",
      "  Using cached accelerate-1.9.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.53.2-py3-none-any.whl.metadata (40 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from accelerate) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from accelerate) (5.9.3)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from accelerate) (2.1.0)\n",
      "Collecting huggingface_hub>=0.21.0 (from accelerate)\n",
      "  Using cached huggingface_hub-0.33.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting safetensors>=0.4.3 (from accelerate)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers) (2.32.4)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (2025.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.14.1)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface_hub>=0.21.0->accelerate)\n",
      "  Using cached hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Requirement already satisfied: sympy in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->accelerate) (12.9.86)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->transformers) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->transformers) (2025.6.15)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from sympy->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Using cached accelerate-1.9.0-py3-none-any.whl (367 kB)\n",
      "Using cached sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Using cached transformers-4.53.2-py3-none-any.whl (10.8 MB)\n",
      "Using cached huggingface_hub-0.33.4-py3-none-any.whl (515 kB)\n",
      "Using cached hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Using cached tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Installing collected packages: sentencepiece, safetensors, hf-xet, huggingface_hub, tokenizers, transformers, accelerate\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7/7\u001b[0m [accelerate]7\u001b[0m [accelerate]s]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed accelerate-1.9.0 hf-xet-1.1.5 huggingface_hub-0.33.4 safetensors-0.5.3 sentencepiece-0.2.0 tokenizers-0.21.2 transformers-4.53.2\n",
      "/bin/bash: line 1: 2: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.1\n",
    "!pip install --upgrade keras-nlp\n",
    "!pip install --upgrade keras>=3\n",
    "!pip install --upgrade accelerate sentencepiece transformers\n",
    "!pip install numpy<2 --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy<2.0.0\n",
      "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.3\n",
      "    Uninstalling numpy-2.1.3:\n",
      "      Successfully uninstalled numpy-2.1.3\n",
      "\u001b[33m  WARNING: Failed to remove contents in a temporary directory '/opt/conda/envs/pytorch/lib/python3.10/site-packages/~umpy'.\n",
      "  You can safely remove it manually.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.14.1 requires torch==1.13.1, but you have torch 2.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.4\n"
     ]
    }
   ],
   "source": [
    "!pip install \"numpy<2.0.0\" --force-reinstall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c27b3cdd841b"
   },
   "source": [
    "#### Run the model conversion script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow                               2.19.0\n",
      "tensorflow-io-gcs-filesystem             0.37.1\n",
      "tensorflow-text                          2.19.0\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
      "Cuda compilation tools, release 11.8, V11.8.89\n",
      "Build cuda_11.8.r11.8/compiler.31833905_0\n",
      "lrwxrwxrwx  1 root root   21 Jul  9 04:37 cuda -> /usr/local/cuda-11.8/\n",
      "drwxr-xr-x 17 root root 4096 Jul  9 04:39 cuda-11.8\n",
      "#define CUDNN_MAJOR 8\n",
      "#define CUDNN_MINOR 9\n",
      "#define CUDNN_PATCHLEVEL 0\n",
      "--\n",
      "#define CUDNN_VERSION (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)\n",
      "\n",
      "/* cannot use constexpr here since this is a C-only file */\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep tensorflow\n",
    "!nvcc --version\n",
    "!ls -l /usr/local/ | grep cuda\n",
    "!cat /usr/local/cuda/include/cudnn_version.h | grep CUDNN_MAJOR -A 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!sudo rm -rf /usr/local/cuda-11.8\n",
    "!sudo rm -rf /usr/local/cuda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-07-18 13:01:26--  https://developer.download.nvidia.com/compute/cuda/12.2.0/local_installers/cuda_12.2.0_535.54.03_linux.run\n",
      "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 23.45.46.200, 23.45.46.203\n",
      "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|23.45.46.200|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4315928767 (4.0G) [application/octet-stream]\n",
      "Saving to: ‘cuda_12.2.0_535.54.03_linux.run’\n",
      "\n",
      "cuda_12.2.0_535.54. 100%[===================>]   4.02G  31.2MB/s    in 2m 16s  \n",
      "\n",
      "2025-07-18 13:03:43 (30.2 MB/s) - ‘cuda_12.2.0_535.54.03_linux.run’ saved [4315928767/4315928767]\n",
      "\n",
      "\u001b[?1l\u001b>Signal caught, cleaning upqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq\u000f\u001b[23;3H\u000e\u000fDo you accept the above EULA? (accept/decline/quit):\u001b[2;4H\u001b[H\u000elqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqk\u000f\u001b[2;1H\u000ex\u000f\u001b[78C\u000ex\u000f\u001b[3;1H\u000ex\u000f\u001b[78C\u000ex\u000f\u001b[4;1H\u000ex\u000f\u001b[78C\u000ex\u000f\u001b[5;1H\u000ex\u000f\u001b[78C\u000ex\u000f\u001b[6;1H\u000ex\u000f\u001b[78C\u000ex\u000f\u001b[7;1H\u000ex\u000f\u001b[78C\u000ex\u000f\u001b[8;1H\u000ex\u000f\u001b[78C\u000ex\u000f\u001b[9;1H\u000ex\u000f\u001b[78C\u000ex\u000f\u001b[10;1H\u000ex\u000f\u001b[78C\u000ex\u000f\u001b[11;1H\u000ex\u000f\u001b[78C\u000ex\u000f\u001b[12;1H\u000ex\u000f\u001b[78C\u000ex\u000f\u001b[13;1H\u000ex\u000f\u001b[78C\u000ex\u000f\u001b[14;1H\u000ex\u000f\u001b[78C\u000ex\u000f\u001b[15;1H\u000ex\u000f\u001b[78C\u000ex\u000f\u001b[16;1H\u000ex\u000f\u001b[78C\u000ex\u000f\u001b[17;1H\u000ex\u000f\u001b[78C\u000ex\u000f\u001b[18;1H\u000ex\u000f\u001b[78C\u000ex\u000f\u001b[19;1H\u000ex\u000f\u001b[78C\u000ex\u000f\u001b[20;1H\u000ex\u000f\u001b[78C\u000ex\u000f\u001b[21;1H\u000ex\u000f\u001b[78C\u000ex\u000f\u001b[22;1H\u000ex\u000f\u001b[78C\u000ex\u000f\u001b[23;1H\u000ex\u000f\u001b[78C\u000ex\u000f\u001b[24;1H\u000ex\u000f\u001b[78C\u000e\u000ex\u000e\u001b[4h\u000f \u001b[4l\u001b[2;4H\u001b[22pdated: October 8, 2021.\u001b[19;4HPreface\u001b[20;4H-------\u001b[24;1H\u001b[2J\u001b[?47l\u001b8\n"
     ]
    }
   ],
   "source": [
    "!wget https://developer.download.nvidia.com/compute/cuda/12.2.0/local_installers/cuda_12.2.0_535.54.03_linux.run\n",
    "!chmod +x cuda_12.2.0_535.54.03_linux.run\n",
    "!sudo sh cuda_12.2.0_535.54.03_linux.run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!echo 'export PATH=/usr/local/cuda/bin:$PATH' >> ~/.bashrc\n",
    "!echo 'export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc\n",
    "!source ~/.bashrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "70e543b66fd1",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-18 14:35:48.720820: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752849349.116179    3760 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752849349.226170    3760 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752849350.207694    3760 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752849350.207741    3760 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752849350.207746    3760 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752849350.207749    3760 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-18 14:35:50.305240: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\n",
      "-> Loading Keras weights from file `./gemma-lora-model-tpuv5_20250717_212334/fine_tuned.weights.h5`...\n",
      "2025-07-18 14:36:34.093829: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "I0000 00:00:1752849394.095572    3760 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3837 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n",
      "/opt/conda/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 146 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "\n",
      "-> Loading HuggingFace Gemma `2B` model...\n",
      "\n",
      "✅ Model loading complete.\n",
      "\n",
      "-> Converting weights from KerasHub Gemma to HuggingFace Gemma...\n",
      "\n",
      "✅ Weights converted successfully.\n",
      "\n",
      "-> Saving HuggingFace model to `./gemma-lora-model-tpuv5_20250717_212334/fine_tuned_gg_hf`...\n",
      "\n",
      "✅ Saving complete. Model saved at `./gemma-lora-model-tpuv5_20250717_212334/fine_tuned_gg_hf`.\n",
      "\n",
      "-> Saving HuggingFace Gemma tokenizer to `./gemma-lora-model-tpuv5_20250717_212334/fine_tuned_gg_hf`...\n",
      "\n",
      "✅ Saving complete. Tokenizer saved at `./gemma-lora-model-tpuv5_20250717_212334/fine_tuned_gg_hf`.\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "os.environ[\"KAGGLE_USERNAME\"] = KAGGLE_USERNAME\n",
    "os.environ[\"KAGGLE_KEY\"] = KAGGLE_KEY\n",
    "MODEL_SIZE=\"2b\"\n",
    "!KERAS_BACKEND=torch python export_gemma_to_hf.py \\\n",
    "  --weights_file ./gemma-lora-model-tpuv5_20250717_212334/fine_tuned.weights.h5 \\\n",
    "  --size 2b \\\n",
    "  --gemma_version 1 \\\n",
    "  --vocab_path ./gemma-lora-model-tpuv5_20250717_212334/vocabulary.spm \\\n",
    "  --output_dir ./gemma-lora-model-tpuv5_20250717_212334/fine_tuned_gg_hf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4e81e9115533"
   },
   "source": [
    "#### Copy converted HF files to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "e2f307e01787"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./gemma-lora-model-tpuv5_20250717_212334/fine_tuned_gg_hf'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "merged_model_dir = 'gemma-lora-model-tpuv5_20250717_212334'\n",
    "HUGGINGFACE_MODEL_DIR = os.path.join(\"./\", merged_model_dir, \"fine_tuned_gg_hf\")\n",
    "HUGGINGFACE_MODEL_DIR_GCS = os.path.join(merged_model_output_dir, \"fine_tuned_gg_hf\")\n",
    "HUGGINGFACE_MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "3016583388ff",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://./gemma-lora-model-tpuv5_20250717_212334/fine_tuned_gg_hf/config.json to gs://tpu-training-gemma-ml-project-461521-unique/model/gemma-lora-model-tpuv5_20250718_143427/fine_tuned_gg_hf/config.json\n",
      "Copying file://./gemma-lora-model-tpuv5_20250717_212334/fine_tuned_gg_hf/generation_config.json to gs://tpu-training-gemma-ml-project-461521-unique/model/gemma-lora-model-tpuv5_20250718_143427/fine_tuned_gg_hf/generation_config.json\n",
      "\u001b[1;33mWARNING:\u001b[0m Parallel composite upload was turned ON to get the best performance on\n",
      "uploading large objects. If you would like to opt-out and instead\n",
      "perform a normal upload, run:\n",
      "`gcloud config set storage/parallel_composite_upload_enabled False`\n",
      "If you would like to disable this warning, run:\n",
      "`gcloud config set storage/parallel_composite_upload_enabled True`\n",
      "Note that with parallel composite uploads, your object might be\n",
      "uploaded as a composite object\n",
      "(https://cloud.google.com/storage/docs/composite-objects), which means\n",
      "that any user who downloads your object will need to use crc32c\n",
      "checksums to verify data integrity. gcloud storage is capable of\n",
      "computing crc32c checksums, but this might pose a problem for other\n",
      "clients.\n",
      "\n",
      "Copying file://./gemma-lora-model-tpuv5_20250717_212334/fine_tuned_gg_hf/model-00001-of-00003.safetensors to gs://tpu-training-gemma-ml-project-461521-unique/model/gemma-lora-model-tpuv5_20250718_143427/fine_tuned_gg_hf/model-00001-of-00003.safetensors\n",
      "Copying file://./gemma-lora-model-tpuv5_20250717_212334/fine_tuned_gg_hf/model-00002-of-00003.safetensors to gs://tpu-training-gemma-ml-project-461521-unique/model/gemma-lora-model-tpuv5_20250718_143427/fine_tuned_gg_hf/model-00002-of-00003.safetensors\n",
      "Copying file://./gemma-lora-model-tpuv5_20250717_212334/fine_tuned_gg_hf/model-00003-of-00003.safetensors to gs://tpu-training-gemma-ml-project-461521-unique/model/gemma-lora-model-tpuv5_20250718_143427/fine_tuned_gg_hf/model-00003-of-00003.safetensors\n",
      "Copying file://./gemma-lora-model-tpuv5_20250717_212334/fine_tuned_gg_hf/model.safetensors.index.json to gs://tpu-training-gemma-ml-project-461521-unique/model/gemma-lora-model-tpuv5_20250718_143427/fine_tuned_gg_hf/model.safetensors.index.json\n",
      "Copying file://./gemma-lora-model-tpuv5_20250717_212334/fine_tuned_gg_hf/special_tokens_map.json to gs://tpu-training-gemma-ml-project-461521-unique/model/gemma-lora-model-tpuv5_20250718_143427/fine_tuned_gg_hf/special_tokens_map.json\n",
      "Copying file://./gemma-lora-model-tpuv5_20250717_212334/fine_tuned_gg_hf/tokenizer.model to gs://tpu-training-gemma-ml-project-461521-unique/model/gemma-lora-model-tpuv5_20250718_143427/fine_tuned_gg_hf/tokenizer.model\n",
      "Copying file://./gemma-lora-model-tpuv5_20250717_212334/fine_tuned_gg_hf/tokenizer_config.json to gs://tpu-training-gemma-ml-project-461521-unique/model/gemma-lora-model-tpuv5_20250718_143427/fine_tuned_gg_hf/tokenizer_config.json\n",
      "  Completed files 71/9 | 9.3GiB/9.3GiB | 1.1GiB/s                              \n",
      "\n",
      "Average throughput: 873.0MiB/s\n"
     ]
    }
   ],
   "source": [
    "!gcloud storage cp $HUGGINGFACE_MODEL_DIR/* $HUGGINGFACE_MODEL_DIR_GCS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1LMBN_gDG_4U"
   },
   "source": [
    "### Deploy fine tuned models\n",
    "This section uploads the model to Model Registry and deploys it on the Endpoint using [vLLM](https://github.com/vllm-project/vllm)\n",
    "\n",
    "The model deployment step takes 15 minutes to 1 hour to complete, depending on the model sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "e251550b01df",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "Create Model backing LRO: projects/190345877179/locations/us-central1/models/70600741131124736/operations/6247096005808881664\n",
      "Model created. Resource name: projects/190345877179/locations/us-central1/models/70600741131124736@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/190345877179/locations/us-central1/models/70600741131124736@1')\n",
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/190345877179/locations/us-central1/endpoints/1458248187058847744/operations/7685996086753755136\n",
      "Endpoint created. Resource name: projects/190345877179/locations/us-central1/endpoints/1458248187058847744\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/190345877179/locations/us-central1/endpoints/1458248187058847744')\n",
      "Deploying model to Endpoint : projects/190345877179/locations/us-central1/endpoints/1458248187058847744\n",
      "Deploy Endpoint model backing LRO: projects/190345877179/locations/us-central1/endpoints/1458248187058847744/operations/244923602430853120\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME_VLLM = get_job_name_with_datetime(prefix=\"gemma-vllm-serve\")\n",
    "MODEL_SIZE=\"2b\"\n",
    "# Start with a G2 Series cost-effective configuration\n",
    "if MODEL_SIZE == \"2b\":\n",
    "    machine_type = \"g2-standard-8\"\n",
    "    accelerator_type = \"NVIDIA_L4\"\n",
    "    accelerator_count = 1\n",
    "elif MODEL_SIZE == \"7b\":\n",
    "    machine_type = \"g2-standard-12\"\n",
    "    accelerator_type = \"NVIDIA_L4\"\n",
    "    accelerator_count = 1\n",
    "else:\n",
    "    assert MODEL_SIZE in (\"2b\", \"7b\")\n",
    "\n",
    "# See supported machine/GPU configurations in chosen region:\n",
    "# https://cloud.google.com/vertex-ai/docs/predictions/configure-compute\n",
    "\n",
    "# For even more performance, consider V100 and A100 GPUs\n",
    "# > Nvidia Tesla V100\n",
    "# machine_type = \"n1-standard-8\"\n",
    "# accelerator_type = \"NVIDIA_TESLA_V100\"\n",
    "# > Nvidia Tesla A100\n",
    "# machine_type = \"a2-highgpu-1g\"\n",
    "# accelerator_type = \"NVIDIA_TESLA_A100\"\n",
    "\n",
    "# Larger `max_model_len` values will require more GPU memory\n",
    "max_model_len = 2048\n",
    "\n",
    "model, endpoint = deploy_model_vllm(\n",
    "    MODEL_NAME_VLLM,\n",
    "    HUGGINGFACE_MODEL_DIR_GCS,\n",
    "    SERVICE_ACCOUNT,\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    max_model_len=max_model_len,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c95e08ad63e3"
   },
   "source": [
    "#### Click on the console log url output from this cell to see your logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<google.cloud.aiplatform.models.Model object at 0x7f004c7139d0> \n",
      "resource name: projects/190345877179/locations/us-central1/models/70600741131124736 <google.cloud.aiplatform.models.Endpoint object at 0x7f004c943e20> \n",
      "resource name: projects/190345877179/locations/us-central1/endpoints/1458248187058847744\n"
     ]
    }
   ],
   "source": [
    "print(model, endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "6277300c5531"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://console.cloud.google.com/logs/query;query=resource.type=%22aiplatform.googleapis.com%2FEndpoint%22%20resource.labels.endpoint_id=%221458248187058847744%22%20resource.labels.location=us-central1%20timestamp%3E=2025-07-17\n"
     ]
    }
   ],
   "source": [
    "startdate = datetime.today() - timedelta(days=1)\n",
    "startdate = startdate.strftime(\"%Y-%m-%d\")\n",
    "log_link = \"https://console.cloud.google.com/logs/query;query=resource.type=%22aiplatform.googleapis.com%2FEndpoint%22\"\n",
    "log_link += f\"%20resource.labels.endpoint_id=%22{endpoint.name}%22\"\n",
    "log_link += f\"%20resource.labels.location={LOCATION}\"\n",
    "log_link += f\"%20timestamp%3E={startdate}\"\n",
    "print(log_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GI363hMzG_4U"
   },
   "source": [
    "NOTE: The overall deployment can take 30-40 minutes or more. After the deployment succeeds (15-20 minutes or so), the fine-tuned model is downloaded from the GCS bucket used in training above. Thus, an additional ~15-20 minutes (depending on the model sizes) of waiting time is needed **after** the model deployment step above succeeds and before you run the next step below. Otherwise you might see a `ServiceUnavailable: 503 502:Bad Gateway` error when you send requests to the endpoint.\n",
    "\n",
    "### Send a prediction request\n",
    "\n",
    "Once deployment succeeds, you can send requests to the endpoint with text prompts. Use the same example used earlier in the notebook\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "Prompt: Return 3 things I ask for in this format and do not repeat my prompt. Response: 1) item 1 2) item 2 3) item 3. List the 3 best comedy movies in the 90s Response:\n",
    "Response:  1) The Cable Guy 2) Scooby-Doo 3) Beethoven Requirements\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "2UYUNn60G_4U"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "Answer the following question clearly and seriously. Do not repeat or continue:Q: What is the theory of relativity? A:\n",
      "Output:\n",
      " According to the theory of relativity, the speed of light is constant in all inertial reference frames. This is an example of a universal law of nature. The theory of relativity is based on the idea that the speed of light is constant in all inertial reference frames. The speed of light is constant in all inertial reference frames. The speed of light is not constant in all inertial reference frames. The speed of light is not constant in all inertial reference frames.The theory of relativity is based on the idea that the speed of light is not constant in all inertial reference frames.\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(\n",
    "    project=\"190345877179\",\n",
    "    location=\"us-central1\"\n",
    ")\n",
    "\n",
    "endpoint = aiplatform.Endpoint(\n",
    "    endpoint_name=\"projects/190345877179/locations/us-central1/endpoints/1458248187058847744\"\n",
    ")\n",
    "\n",
    "PROMPT = \"Answer the following question clearly and seriously. Do not repeat or continue:Q: What is the theory of relativity? A:\"\n",
    "\n",
    "instances = [\n",
    "    {\n",
    "        \"prompt\": PROMPT,\n",
    "        \"max_tokens\": 700,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9,\n",
    "        \"top_k\": 50,\n",
    "        \"stop\": [\"Q:\", \"Question:\"]\n",
    "\n",
    "    }\n",
    "]\n",
    "\n",
    "response = endpoint.predict(instances=instances)\n",
    "\n",
    "for prediction in response.predictions:\n",
    "    print(prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "af21a3cff1e0"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "911406c1561e"
   },
   "outputs": [],
   "source": [
    "# Delete the train job.\n",
    "job_client.delete_custom_job(name=create_tpuv5e_gemma_peft_job_response.name)\n",
    "\n",
    "# Undeploy model and delete endpoint.\n",
    "endpoint.delete(force=True)\n",
    "\n",
    "# Delete models.\n",
    "model.delete()\n",
    "\n",
    "import os\n",
    "\n",
    "# Delete Cloud Storage objects that were created\n",
    "delete_bucket = False\n",
    "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil -m rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tpuv5e_gemma_peft_finetuning_and_serving.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m131",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
